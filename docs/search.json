[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EA Market testing data analysis",
    "section": "",
    "text": "May want to build Flexdashboards, Htmlwidgets are easy and useful; see also Shiny packages. DT::datatable where possible, style as appropriate↩︎\nHowever, we’re not sure yet if and how it can be integrated with the https://app.gitbook.com/ content.↩︎\nNote: as this is Quarto and not Rmd, packages need to be loaded in every chapter. I’ll put these in code/shared_packages_code.R.↩︎"
  },
  {
    "objectID": "chapters/gwwc_gg.html",
    "href": "chapters/gwwc_gg.html",
    "title": "1  Giving What We Can: Giving guides",
    "section": "",
    "text": "Note\n\n\n\nThis chapter aligns with, and supplements the EA Forum post “Marketing messages trial for GWWC Giving Guide Campaign”"
  },
  {
    "objectID": "chapters/gwwc_gg.html#quick-overview",
    "href": "chapters/gwwc_gg.html#quick-overview",
    "title": "1  Giving What We Can: Giving guides",
    "section": "1.1 Quick overview",
    "text": "1.1 Quick overview\n\nContext: Facebook ads on a range of audiences\n… [with text and rich content promoting effective giving and a “giving guide” – links people to a Giving What We Can page asking for their email in exchange for the guide]\nObjective: Test distinct approaches to messaging, aiming to get people to download our Giving Guide. (Previously called “emotional and factual” approaches; basically these were “Charity research facts” vs. “cause focus”)\nProcess and further goals: GWWC worked with a creative agency to develop a set of animated video ads wiht differing texts. This work is also informative about costs and the ‘value of targeting’ in this context. Note these ads had several ‘versions’ during the campaign (see ‘Test 1 - Test 4’).\nKey findings:\n\nThe cost of an email address acquired via a Facebook campaign during Giving Season was as low as $8.00 across campaigns but could be lower with more targeting.\n“Only 3% of people give effectively,” seems to be an effective message for generating link clicks and email addresses vs. the other messages.\nLookalike and animal rights audiences seem the most promising audiences.\nDemographics are not very predictive on a per-$ basis.\n\nKey caveats\n\nSpecificity and interpretation: All comparisons are not for ‘audiences of similar composition’ but for ‘the best audience Facebook could find to show the ads, within each group, according to its algorithm’. Thus differences in performance may combine ‘better targeting’ with ‘better performance on the targeted group’. See our detailed discussion HERE.\n\nI.e., we can make statements about “what works better on Facebook in this context and maybe similar contexts”\nbut not about “which audience, as defined, is more receptive” (as the targeting within each audience may differ in unobserved ways),\nnor about “which message works better on a particular comparable audience” (for the same reason)\n\nOutcome is ‘click to download the giving guide’.\n\n\n\nInstalling BiocManager [1.30.20] ...\n    OK [linked cache]\n\n\n\n\nCode: Import raw data for descriptions\n#importing for dynamic input of descriptions below\n\n#Todo: get direct import working: See https://cran.r-project.org/web/packages/rfacebookstat/rfacebookstat.pdf\n\nraw_data_path <- list(\"gwwc\", \"gg_raw_data_shareable\")\n\nmini_clean <- function(df){\n  df %>%\n    as_tibble() %>%\n  janitor::clean_names() %>%\n  janitor::remove_empty() %>% # removes empty rows and columns, here `unique_link_clicks`\n    janitor::remove_constant()\n    }\n\ngg_campaign_by_ad_by_text <- read_csv(here(raw_data_path, \"gg_campaign_by_ad_by_text.csv\"), show_col_types=FALSE) %>%\n  dplyr::select(-\"Campaign name...4\") %>% #duplicate columns?\n mini_clean"
  },
  {
    "objectID": "chapters/gwwc_gg.html#the-trial",
    "href": "chapters/gwwc_gg.html#the-trial",
    "title": "1  Giving What We Can: Giving guides",
    "section": "1.2 The trial",
    "text": "1.2 The trial\nContext: Facebook advertisements with a range of audiences/filters1\n\nEffective Giving Guide Lead Generation campaign … ran late November 2021 - January 2022. The objective of this campaign was to see whether a factual [‘who researches giving’ or ‘magnitude of impact differences’] or cause-led approach was more cost-effective at getting people to fill out a form and give us their email in order to download our Effective Giving Guide.\n\nThe page they were directed to,  “Learn how to give more effectively” allowed you to download the guide only after leaving your email.\n\nTreatments (text \\(\\times\\) video)\nThere were two dimensions of treatment content:2\n\nThe texts displayed above the videos\n\n\n\n\n\n\n\nTexts\n\n\n\n\n\nBigger difference next year: Want to make a bigger difference next year? Start with our Effective Giving Guide and learn how to make a remarkable impact just by carefully choosing the charities you give to.\n100x impact: Did you know that the best charities can have a 100x greater impact? Download our free Effective Giving Guide for the best tips on doing the most good this holiday season.\n6000 people: Giving What We Can has helped 6,000+ people make a bigger impact on the causes they care about most. Download our free guide and learn how you can do the same.\nCause list: Whether we’re moved by animal welfare, the climate crisis, or worldwide humanitarian efforts, our community is united by one thing: making the biggest impact we can. Make a bigger difference in the world through charitable giving. Start by downloading our Effective Giving Guide. You’ll learn how to approach charity research and smart giving. And be sure to share it with others who care about making a greater impact on the causes closest to their hearts.\nLearn: Use our free guide to learn how to make a bigger impact on the causes you care about most.\nOnly 3% research: Only 3% of donors give based on charity effectiveness yet the best charities can be 100x more impactful. That’s incredible! Check out the Effective Giving Guide 2021. It’ll help you find the most impactful charities across a range of causes.\nOverwhelming: It can be overwhelming with so many problems in the world. Fortunately, we can do a lot to help, if we give effectively. Check out the Effective Giving Guide 2021. It’ll help you find the most impactful charities across a range of causes.\n\n\n\n\n\n\n\n\n\nFull text: exported from data set\n\n\n\n\n\nGiving What We Can has helped 6,000+ people make a bigger impact on the causes they care about most. Download our free guide and learn how you can do the same., It can be overwhelming with so many problems in the world. Fortunately, we can do a lot to help, if we give effectively.\nCheck out the Effective Giving Guide 2021. It’ll help you find the most impactful charities across a range of causes., Use our free guide to learn how to make a bigger impact on the causes you care about most., Want to make a bigger difference next year? Start with our Effective Giving Guide and learn how to make a remarkable impact just by carefully choosing the charities you give to., Whether we’re moved by animal welfare, the climate crisis, or worldwide humanitarian efforts, our community is united by one thing: making the biggest impact we can.\nMake a bigger difference in the world through charitable giving. Start by downloading our Effective Giving Guide. You’ll learn how to approach charity research and smart giving. And be sure to share it with others who care about making a greater impact on the causes closest to their hearts., Did you know that the best charities can have a 100x greater impact? Download our free Effective Giving Guide for the best tips on doing the most good this holiday season., Only 3% of donors give based on charity effectiveness yet the best charities can be 100x more impactful. That’s incredible!\nCheck out the Effective Giving Guide 2021. It’ll help you find the most impactful charities across a range of causes.\n\n\n\n\nThe Video ads theme and content\n\n\n\n\n\n\n\n“Facts”\n\n\n\n\n\n\nCharity research facts short video (8 seconds): Only 3% of donors research charity effectiveness, yet the best charities can 100x your impact, learn how to give effectively \nCharity research facts long video (22 seconds): Trivial things we search (shows someone searching how to do Gangnam style), things we should research (shows someone searching how to donate effectively), only 3% of donors research charity effectiveness, yet the best charities can 100x your impact, learn how to give effectively. Slower paced music compared to the short video and cause videos.  \n\n\n\n\n\n\n\n\n\n\n“Cause focus”\n\n\n\n\n\n\nClimate change (15 seconds): Care about climate change? You don’t have to renounce all your possessions, But you could give to effective environmental charities, Learn how to maximize your charitable impact, Download the Effective Giving Guide \nAnimal welfare (16 seconds): Care about animals? You don’t have to adopt 100 cats, But you could give to effective animal charities, Learn how to maximize your charitable impact, Download the Effective Giving Guide \nPoverty (16 seconds): Want to help reduce global poverty? You don’t have to build a village, But you could give to effective global development charities, Learn how to maximize your charitable impact, Download the Effective Giving Guide \n\n\n\n\n\n\n\n\n\n\nArguments, rich content from “Brand Video”\n\n\n\n\n\n\nBrand Video (1 min 22 seconds): Animated and voiceover video that explains how GWWC can help maximize charitable impact (support, community, and information) and the problems GWWC addresses (good intentions don’t always produce the desired outcomes, there are millions of charities that have varying degrees of impact and some can even cause harm). CTA: Check out givingwhatwecan.org to learn how you can become an effective giver.\n\n\n\n\n\n\nFurther detail, links\n\n\n\n\n\n\nNotes from the trial description\n\n\n\n\n\n“In the original version of our test, we had 1 video for the factual appeal and 3 videos for the cause led approach - 1 for global health and development, 1 for animal welfare and 1 for climate change.”\n“We targeted our ads to audiences we thought were likely to engage based on their interests and demographics, and targeted the cause led videos to a relevant audience, i.e. climate change message to climate change audience.”\n“We also had various text above the videos that were displayed and optimized.”\n\n\n\nDetails in Gitbook HERE (and embedded below) and Gdoc here\n\n\nCode\nknitr::include_url(\"https://effective-giving-marketing.gitbook.io/untitled/partner-organizations-and-trials/gwwc/giving-guides-+\")\n\n\n\n\n\n\nImplementation & assignment: key details\n\n\n\n\n\n\nTreatment assignment order, dates\n\n\n\n\n\nThe treatment assignment was determined by Facebook’s algorithm. Video content was manipulated across three split tests.\nTest 1 (Nov 30, 2021 – Dec 8, 2021, campaigns: “Cause-Led” and “Factual”) displayed either the long factual video or a cause focus video. In the cause focus condition, cause-specific audiences for animal rights, climate change, and poverty (based on their behavior on Facebook) were shown the relevant cause video.\nTest 2 (Starting December 8, campaign “Factual V2”?) was the same as Test 1 but used the short factual video instead of the cause-focus videos.\nTest 3 (Starting December 23, campaign “Cause-led V3” and “Factual V3” (?)): was the same as Test 2 but had a new version of the videos (with Luke just holding up signs with the words). This test was also restricted to 18-35 (or 18-44) year olds.3\nTest 4: (Starting December 23, “Brand Video” and “PPCo”) The Brand Video was displayed in a separate campaign which was tested against another campaign that allowed the algorithm to optimize between the ‘short factual’ and ‘cause focus’ videos (although not allowing each cause-specific audience to see the ads for other cause areas).\nIn all tests, the text content displayed above the video was determined by Facebook’s algorithm. Balance across variations was determined to equate budgets across split tests; otherwise, according to Facebook’s algorithm. All variation was done at the level of the impression.\nThe videos were adapted across the trials as we learned. First, we updated the factual video to be shorter for Trial 2, and then we tried videos of Luke holding up signs spelling out the voiceover in Trial 3 for all videos."
  },
  {
    "objectID": "chapters/gwwc_gg.html#build-source-data-cleaning",
    "href": "chapters/gwwc_gg.html#build-source-data-cleaning",
    "title": "1  Giving What We Can: Giving guides",
    "section": "1.3 Build: Source data, cleaning",
    "text": "1.3 Build: Source data, cleaning\n\n\n\n\n\n\nAccessing, downloading, inputting data\n\n\n\n\n\nSee:\nhttps://effective-giving-marketing.gitbook.io/untitled/marketing-and-testing-opportunities-tools-tips/collecting-data-trial-outcomes/facebook-meta-ads-interface\nAccessing and bringing down simple results HERE\nWe import the exported ‘pivot table’ gg_campaign_by_ad_by_text below, as well as the more detailed version, broken down by age range and gender: gg-campaign-by-ad-set-text-age-gender.csv.4\n\n\n\n\n\n\n\n\n\n\n\nData structure\n\n\n\n\n\nThe data frame gg_campaign_by_ad_by_text_age_gender has one row per combination of ‘campaign, ad, text, age group, gender’.\nEach row represents a combination of the below (with different numbers of ‘reach’ for each row)5\n\ncampaign_name: When and and with what funds the ad was launched, I think (?)\nad_set: An ad set can specifically tie an ad_name to an audience (I think)\nad_name: Which video/media (or collection of optimized videos/media) was shown; note this is paired with ‘which audience’ in it’s label, as there were specific ‘global poverty’, ‘animal welfare’, ‘climate change’, ‘philanthropy’ and ‘retargeting’ audiences\n\nCaveat: The ad_name seems to select from a different set of media for optimization depending on which ad_set it is in.6\n\ntext: Which text was shown along with the video\nage (a range of ages)\ngender (female, male, unknown)\n\n\n\n\n\n\nImport raw data\nraw_data_path <- list(\"gwwc\", \"gg_raw_data_shareable\")\n\n#already input above: gg_campaign_by_ad_by_text\n\n#Version allowing demographic breakdown:\ngg_campaign_by_ad_by_text_age_gender <- read_csv(here(raw_data_path, \"gg-campaign-by-ad-set-text-age-gender.csv\"), show_col_types=FALSE) %>%\n  #dplyr::select(-\"Campaign name...4\") %>% #duplicate columns?\n mini_clean()\n\n#Version with information on cause videos shown (even to those in 'general' groups):\n\ngg_video_breakdowns <- read_csv(here(raw_data_path, \"gg-image-video-breakdowns.csv\"), show_col_types=FALSE)\n\n#capture and remove columns that are the same everywhere\nattribution_setting_c <- gg_campaign_by_ad_by_text_age_gender$attribution_setting %>% .[1]\nreporting_starts_c <- gg_campaign_by_ad_by_text_age_gender$reporting_starts %>% .[1]\nreporting_ends_c <- gg_campaign_by_ad_by_text_age_gender$reporting_ends %>% .[1]\n\ngg_campaign_by_ad_by_text_age_gender  %<>% mini_clean()\ngg_video_breakdowns  %<>% mini_clean()\n\n#functions to clean these specific data sets 'gg_campaign_by_ad_by_text_age_gender' and 'gg_campaign_by_ad_by_text':\nsource(here(\"gwwc\", \"giving_guides\", \"clean_gg_raw_data.R\"))\n\n#Many cleaning steps: audience, video_theme, campaign_theme, agetrin; releveling\ngg_campaign_by_ad_by_text_age_gender %<>%\n  rename_gg() %>%\ngg_make_cols() %>%\n  text_clean() %>%  # Shorter 'text treatment' column\n  dplyr::select(campaign_name, everything(), -campaign_name_1, -campaign_name_7) #campaign_name_7 was the same as campaign_name_1\n\ngg_video_breakdowns %<>%\n  rename_gg() %>%\ngg_make_cols()\n\n#gg_campaign_by_ad_by_text_age_gender %>% collapse::descr()\n\n\n\n\nEncode video types\ngg_video_breakdowns %<>%\n  mutate(\n    video_theme = case_when(\n      str_det(image_video_and_slideshow, \"set_1|Animals\") ~ \"Animal\",\n      str_det(image_video_and_slideshow, \"set_2|Climate\") ~ \"Climate\",\n      str_det(image_video_and_slideshow, \"set_3|Poverty\") ~ \"Poverty\",\n      str_det(image_video_and_slideshow, \"Free Effective\") & video_theme==\"Cause-led (any)\" ~ \"Poverty\",\n      str_det(image_video_and_slideshow, \"factual_short|Factual Short\") ~ \"Factual short\",\n    TRUE ~   video_theme\n    ),\n    video_theme =  factor(video_theme),\n    video_theme = fct_relevel(video_theme, c(\"Animal\", \"Climate\", \"Poverty\", \"Factual short\", \"Factual long\", \"Branded (factual)\"))\n  )\n\n\n\n\nExport/save data\ncleaned_data_path <- list(\"gwwc\", \"gg_processed_data\")\n\n#export 'cleaned' data for others to play with immediately\nwrite.csv(gg_video_breakdowns, here(cleaned_data_path, \"gg_video_breakdowns.csv\"), row.names = FALSE)\nwrite.csv(gg_campaign_by_ad_by_text_age_gender, here(cleaned_data_path, \"gg_campaign_by_ad_by_text_age_gender.csv\"), row.names = FALSE)\n\nwrite_rds(gg_video_breakdowns, here(cleaned_data_path, \"gg_video_breakdowns.Rdata\"))\nwrite_rds(gg_campaign_by_ad_by_text_age_gender, here(cleaned_data_path, \"gg_campaign_by_ad_by_text_age_gender.Rdata\"))\n\n\n\n\n\n\n\n\nThis data is being publicly shared; above, ‘export’ above\n\n\n\n\n\nThis data is clearly not identifying individuals; it involves aggregates based on real or assumed characteristics … there is likely nothing that needs to be hidden here. We are sharing and integrating all the data in this repo, for a complete pipeline.\nIn the code above, we create several ‘cleaned data’ files for others to access in the linked Github repo (under gwwc.)\nYou can access the original data referred to above, and/or fork or clone our public Github repository and run the present code if you like.\n\n\n\n\n\n\n\n\n\nPrevious version of data used … (moved)\n\n\n\n\n\nWe previously used data collapsed (breakdowns) by demography and ad set, into 2 files, which duplicated rows to represent the number of people reached: video breakdown, and text breakdown.csv. We now use the more ‘raw’ minimal version of the data, avoiding duplicating rows where possible.\nBelow, we also input some of the ‘old version’ of the data, with the duplicated rows, to accommodate the old-format of analysis … this will be removed when we switch over. The code above inputs and builds 2-4 related data frames (tibbles), which were constructed from the collapsed (aggregated) data by multiplying rows according to observation counts. I am not sure where this was done. Once we update the rest we will get rid of this. …\ne.g.,\ngwwc_vid_results: Observations of emails provided… by video content\nThis content/input has been moved to eamt_data_analysis/gwwc/giving_guides/archive_erin/erin_plots_stats_gg.qmd, at least for now"
  },
  {
    "objectID": "chapters/gwwc_gg.html#descriptives",
    "href": "chapters/gwwc_gg.html#descriptives",
    "title": "1  Giving What We Can: Giving guides",
    "section": "1.4 Descriptives",
    "text": "1.4 Descriptives\n\n1.4.1 Implemented treatments, ‘reach’ {-reach}\nFirst we illustrate ‘where, when, and to whom’ the different campaigns and treatments were shown (‘people reached’ on Facebook).\n\n\n\n\n\n\nWe use ‘reach’ as our metric throughout’; essentially ‘unique impressions’\n\n\n\n\n\nAccording to Facebook (https://business.facebook.com/adsmanager/ accessed 5 Aug 2022), reach is:\n\nThe number of people who saw your ads at least once. Reach is different from impressions, which may include multiple views of your ads by the same people.\n\nIn all of our figures below we use the reach outcome rather than the ‘impressions’ outcome, although we sometimes refer to it as ‘impressions’, because it is a clearer way of describing it.\n\n\n\nThe sequential campaigns involved different sets of videos.\n\n\nCode\n(\n  reach_campaign_theme <- gg_video_breakdowns %>%\n  dplyr::select(campaign_name, video_theme, reach) %>%\n   uncount(weights = .$reach) %>%\n    dplyr::select(-reach) %>%\n   tabyl(campaign_name, video_theme) %>%\n    dplyr::select(campaign_name, Animal, Climate, Poverty, everything()) %>%\n  .kable(caption = \"Campaign names and video themes: unique impressions\") %>%\n  .kable_styling()\n)\n\n\n\n\nCampaign names and video themes: unique impressions\n \n  \n    campaign_name \n    Animal \n    Climate \n    Poverty \n    Factual short \n    Factual long \n    Branded (factual) \n  \n \n\n  \n    Branded \n    0 \n    0 \n    0 \n    0 \n    0 \n    56,694 \n  \n  \n    Giving Guide 2021 - Cause-led \n    81,298 \n    5,272 \n    33,027 \n    0 \n    0 \n    0 \n  \n  \n    Giving Guide 2021 – Cause-led V3 \n    37,680 \n    3,882 \n    60,346 \n    0 \n    0 \n    0 \n  \n  \n    Giving Guide 2021 – Factual \n    0 \n    0 \n    0 \n    2,333 \n    25,316 \n    0 \n  \n  \n    Giving Guide 2021 – Factual V2 \n    0 \n    0 \n    0 \n    103,329 \n    0 \n    0 \n  \n  \n    Giving Guide 2021 – Factual V3 \n    0 \n    0 \n    0 \n    112,203 \n    0 \n    0 \n  \n  \n    Giving Guide 2021 – PPCo Creatives \n    23,037 \n    23,238 \n    2,383 \n    62,116 \n    0 \n    0 \n  \n\n\n\n\n\nCode\nreach_campaign_audience <- gg_campaign_by_ad_by_text_age_gender %>% #created but not shown for now\n  dplyr::select(campaign_name, audience, reach) %>%\n   uncount(weights = .$reach) %>%\n    dplyr::select(-reach) %>%\n   tabyl(campaign_name, audience) %>%\n  .kable(caption = \"Campaign names and audiences: unique impressions\") %>%\n  .kable_styling()\n\n\n\n\n\n\n\n\nThese videos had different versions\n\n\n\n\n\n\n\nCode\n(\n  versions_of_videos <- gg_campaign_by_ad_by_text_age_gender %>%\n  dplyr::select(video_theme, version, reach) %>%\n   uncount(weights = .$reach) %>%\n    dplyr::select(-reach) %>%\n   table %>%\n  .kable(caption = \"Versions of videos: unique impressions\") %>%\n  .kable_styling()\n)\n\n\n\n\nVersions of videos: unique impressions\n \n  \n      \n    V1 \n    V2 - factual shortened \n    V3 - sometimes Luke \n    Video/creatives \n  \n \n\n  \n    Animal \n    40,852 \n    0 \n    20,593 \n    0 \n  \n  \n    Branded (factual) \n    0 \n    0 \n    0 \n    66,393 \n  \n  \n    Cause-led (any) \n    79,624 \n    0 \n    75,570 \n    117,888 \n  \n  \n    Climate \n    1,779 \n    0 \n    339 \n    0 \n  \n  \n    Factual long \n    29,589 \n    0 \n    0 \n    0 \n  \n  \n    Factual short \n    0 \n    109,262 \n    120,244 \n    0 \n  \n  \n    Poverty \n    7,906 \n    0 \n    12,017 \n    0 \n  \n\n\n\n\n\n\n\n\n#### Which videos were shown to which audiences? {-which_videos}\nSome audiences were profiled as being associated with a certain cause (through their Facebook interests or activities): in ‘cause-focused’ campaigns they were shown videos for their profiled cause. In campaigns that were not cause-focused, they were shown general interest videos. However, those associated with one cause were never shown videos for other causes.\nAudiences not associated with a cause included the ‘General’ audience, the Philanthropy (interested in charity) audience, a GWWC ‘Lookalike’ audience, and a Retargeted audience: these audiences were shown either the more general-interest videos or particular cause videos.7 This is illustrated in the table below.\n\n\n\nCode\nvideo_levels <- c(\"Animal\", \"Climate\", \"Poverty\", \"Factual short\",  \"Factual long\", \"Branded (Factual)\", \"Total\")\n\naudience_levels <- c(\"Animal\", \"Climate\", \"Global Poverty\", \"Philanthropy\", \"General audience\", \"Lookalikes\", \"Retargeting\")\n\n\nadorn_opts <- function(df) {\n  df %>% \n    adorn_percentages(\"all\") %>%\n    adorn_totals(where = c(\"row\", \"col\")) %>%\n    adorn_pct_formatting(digits = 2)  \n}\n\n(\nreach_video_audience <- gg_video_breakdowns %>%\n    dplyr::select(video_theme, audience, reach) %>%\n    uncount(weights = .$reach) %>%\n    dplyr::select(-reach) %>%\n    tabyl(audience, video_theme) %>%\n    adorn_opts() %>% \n    dplyr::select(audience, Animal, Climate,Poverty, everything()) %>%\n   mutate(audience =  factor(audience, levels = audience_levels)) %>%\n  arrange(audience)  %>%\n    .kable(caption = \"Video themes (columns) by audience (rows): share of unique impressions\", digits=3) %>%\n  .kable_styling() \n\n\n)\n\n\n\n\nVideo themes (columns) by audience (rows): share of unique impressions\n \n  \n    audience \n    Animal \n    Climate \n    Poverty \n    Factual short \n    Factual long \n    Branded (factual) \n    Total \n  \n \n\n  \n    Animal \n    9.77% \n    0.00% \n    0.00% \n    11.84% \n    1.22% \n    0.95% \n    23.78% \n  \n  \n    Climate \n    0.00% \n    1.80% \n    0.00% \n    12.53% \n    1.19% \n    2.26% \n    17.78% \n  \n  \n    Global Poverty \n    0.00% \n    0.00% \n    3.04% \n    4.88% \n    0.64% \n    1.05% \n    9.60% \n  \n  \n    Philanthropy \n    10.02% \n    1.88% \n    9.26% \n    8.19% \n    0.88% \n    1.88% \n    32.11% \n  \n  \n    General audience \n    1.35% \n    0.98% \n    0.18% \n    2.11% \n    0.00% \n    2.67% \n    7.30% \n  \n  \n    Lookalikes \n    1.31% \n    0.45% \n    2.66% \n    4.73% \n    0.08% \n    0.16% \n    9.39% \n  \n  \n    Retargeting \n    0.01% \n    0.01% \n    0.00% \n    0.01% \n    0.00% \n    0.01% \n    0.04% \n  \n  \n    NA \n    22.47% \n    5.12% \n    15.15% \n    44.29% \n    4.00% \n    8.97% \n    100.00% \n  \n\n\n\n\n\n\nBelow (in fold), we see that the second treatment dimension – the text presented along with the video – was allowed to vary independently of the video (but these are not ‘statistically independent’).\n\n\n\n\n\n\nVideo themes by text treatment\n\n\n\n\n\n\n\nCode\nvideo_levels_gg <- c(\"Animal\", \"Climate\", \"Poverty\", \"Cause-led (any)\", \"Factual short\",  \"Factual long\", \"Branded (Factual)\", \"Total\")\n\n(\n  reach_video_text <- gg_campaign_by_ad_by_text_age_gender %>%\n  dplyr::select(video_theme, text_treat, reach) %>%\n   uncount(weights = .$reach) %>%\n    dplyr::select(-reach) %>%\n        tabyl(video_theme, text_treat) %>%\n       adorn_opts() %>% \n   mutate(video_theme =  factor(video_theme, levels = video_levels_gg)) %>%\n  arrange(video_theme)  %>%\n  .kable(caption = \"Video themes by text treatment: unique impressions\", digits=3) %>%\n  .kable_styling()\n)\n\n\n\n\nVideo themes by text treatment: unique impressions\n \n  \n    video_theme \n    100x impact \n    6000+ people \n    Bigger difference \n    Cause list \n    Learn \n    Only 3% research \n    Overwhelming \n    Total \n  \n \n\n  \n    Animal \n    0.00% \n    2.13% \n    3.36% \n    1.08% \n    1.26% \n    0.00% \n    1.18% \n    9.01% \n  \n  \n    Climate \n    0.00% \n    0.04% \n    0.05% \n    0.04% \n    0.13% \n    0.00% \n    0.05% \n    0.31% \n  \n  \n    Poverty \n    0.00% \n    0.40% \n    0.83% \n    1.01% \n    0.37% \n    0.00% \n    0.32% \n    2.92% \n  \n  \n    Cause-led (any) \n    4.66% \n    8.59% \n    5.94% \n    3.49% \n    6.79% \n    3.39% \n    7.19% \n    40.04% \n  \n  \n    Factual short \n    9.51% \n    5.39% \n    7.37% \n    0.00% \n    4.73% \n    6.64% \n    0.00% \n    33.65% \n  \n  \n    Factual long \n    0.61% \n    0.83% \n    0.94% \n    0.00% \n    1.28% \n    0.68% \n    0.00% \n    4.34% \n  \n  \n    Total \n    17.47% \n    18.81% \n    20.19% \n    5.62% \n    16.15% \n    13.04% \n    8.74% \n    100.00% \n  \n  \n    NA \n    2.69% \n    1.43% \n    1.71% \n    0.00% \n    1.59% \n    2.32% \n    0.00% \n    9.73% \n  \n\n\n\n\n\nNote (again) that we cannot identify all of the video treatments in the same dataset with text treatments; thus, some are characterized as ‘cause-led (any)’. This is a limitation of the Facebook interface.\n\n\n\nNote that treatment shares are not equal. In fact, as the first table in this section shows, they are not even equal within each campaign. This is because Facebook optimizes to show more succesful videos and text more than less succesful versions.8\nAs shown in the fold below, the set of text treatments varied across campaigns:\n\n\n\n\n\n\nText treatments by campaign\n\n\n\n\n\nBelow, we present the text treatments as shares of each campaign’s unique impressions. We see that the text treatments varied as shares of the treatments in each campaign. Some texts were swapped for other texts in later campaigns. But even among campaigns that used the same overall set of texts, there was some dramatic variation E.g., the ‘100x impact’ was favored heavily in the ‘Factual V2’ campaign, while the other ‘Factual’ campaigns used this much less frequently. This presumably resulted from it performing better in the earliest hours of the Factual V2 trial, and Facebook’s algorithm thus favoring it.9\n\n\nCode\n(\n  reach_text_campaign <- gg_campaign_by_ad_by_text_age_gender %>%\n  dplyr::select(campaign_name, text_treat, reach) %>%\n   uncount(weights = .$reach) %>%\n    dplyr::select(-reach) %>%\n tabyl(campaign_name, text_treat) %>%\n    adorn_percentages(\"row\") %>%\n     adorn_totals(where = c(\"col\")) %>%\n    adorn_pct_formatting(digits = 2) %>%\n  .kable(caption = \"Text treatments as shares of unique impressions by campaign\", digits=3) %>%\n  .kable_styling()\n)\n\n\n\n\nText treatments as shares of unique impressions by campaign\n \n  \n    campaign_name \n    100x impact \n    6000+ people \n    Bigger difference \n    Cause list \n    Learn \n    Only 3% research \n    Overwhelming \n    Total \n  \n \n\n  \n    Branded \n    27.61% \n    14.65% \n    17.55% \n    0.00% \n    16.37% \n    23.82% \n    0.00% \n    100.00% \n  \n  \n    Cause-led \n    0.00% \n    24.99% \n    27.61% \n    15.45% \n    13.15% \n    0.00% \n    18.79% \n    100.00% \n  \n  \n    Cause-led V3 \n    0.00% \n    23.91% \n    11.08% \n    16.76% \n    15.88% \n    0.00% \n    32.36% \n    100.00% \n  \n  \n    Factual \n    14.05% \n    19.08% \n    21.65% \n    0.00% \n    29.49% \n    15.73% \n    0.00% \n    100.00% \n  \n  \n    Factual V2 \n    28.16% \n    10.41% \n    21.63% \n    0.00% \n    11.88% \n    27.93% \n    0.00% \n    100.00% \n  \n  \n    Factual V3 \n    28.37% \n    21.14% \n    22.14% \n    0.00% \n    16.04% \n    12.31% \n    0.00% \n    100.00% \n  \n  \n    PPCo Creatives \n    26.94% \n    14.95% \n    18.16% \n    0.00% \n    20.34% \n    19.61% \n    0.00% \n    100.00% \n  \n\n\n\n\n\n\n\n\n\n\nDemographics\n\n\nCode\n(\n  reach_age_gender <- gg_campaign_by_ad_by_text_age_gender %>%\n  dplyr::select(age, gender, reach) %>%\n   uncount(weights = .$reach) %>%\n    dplyr::select(-reach) %>%\n  tabyl(age, gender) %>%\n       adorn_opts() %>% \n  .kable(caption = \"Unique impressions: shares by Age and Gender\", digits=2) %>%\n  .kable_styling()\n)\n\n\n\n\nUnique impressions: shares by Age and Gender\n \n  \n    age \n    female \n    male \n    unknown \n    Total \n  \n \n\n  \n    13-17 \n    0.03% \n    0.02% \n    0.01% \n    0.05% \n  \n  \n    18-24 \n    13.02% \n    5.78% \n    0.63% \n    19.43% \n  \n  \n    25-34 \n    27.50% \n    9.32% \n    0.68% \n    37.50% \n  \n  \n    35-44 \n    15.71% \n    4.27% \n    0.47% \n    20.45% \n  \n  \n    45-54 \n    5.21% \n    0.95% \n    0.15% \n    6.30% \n  \n  \n    55-64 \n    6.42% \n    1.26% \n    0.16% \n    7.84% \n  \n  \n    65+ \n    6.44% \n    1.78% \n    0.20% \n    8.42% \n  \n  \n    Total \n    74.32% \n    23.39% \n    2.29% \n    100.00% \n  \n\n\n\n\n\nAs can be clearly seen above, within all age groups, the ads were disproportionally shown to women. Relative to the overall Facebook population our data skews very slightly younger.10\n\n\n\nOutcomes: overview\nBelow, we present the dates of each campaign, along with start dates and results:\n\n\n\nCode\nbase_results_sum <- function(df) {\n    df %>%\n     dplyr::summarize(\n  Cost = sum(round(amount_spent_usd,0)),\n      `reach`=sum(reach),\n      `Link clicks`=sum(link_clicks, na.rm = TRUE),\n      Results=sum(results, na.rm = TRUE),\n      `$/ impr.` = round(Cost/reach,3),\n      `$/ click` = round(Cost/ `Link clicks`,1),\n      `$/ result` = round(Cost/Results,1),\n      `Results/ 1k impr.` = round(Results*1000/reach,1)\n)\n     }\n\n(\n  campaign_date_outcomes <-  gg_campaign_by_ad_by_text_age_gender %>%\n    group_by(campaign_name, starts, ends) %>%\n    rename('Campaign' = campaign_name) %>%\n    filter(reach>200) %>%\n    base_results_sum %>%\n    arrange(starts) %>%\n    .kable(caption = \"Results by Campaign and start date\") %>%\n    .kable_styling() %>%\n    add_footnote(\"'False start' campaign dates with less than 200 reach are excluded\")\n)\n\n\n\n\nResults by Campaign and start date\n \n  \n    Campaign \n    starts \n    ends \n    Cost \n    reach \n    Link clicks \n    Results \n    $/ impr. \n    $/ click \n    $/ result \n    Results/ 1k impr. \n  \n \n\n  \n    Cause-led \n    2021-11-30 \n    2021-12-20 \n    4,451 \n    113,399 \n    1,111 \n    407 \n    0.039 \n    4.0 \n    10.9 \n    3.6 \n  \n  \n    Factual \n    2021-11-30 \n    2021-12-08 \n    484 \n    10,752 \n    94 \n    19 \n    0.045 \n    5.1 \n    25.5 \n    1.8 \n  \n  \n    Factual V2 \n    2021-12-08 \n    2021-12-20 \n    3,401 \n    94,168 \n    1,362 \n    417 \n    0.036 \n    2.5 \n    8.2 \n    4.4 \n  \n  \n    Cause-led V3 \n    2021-12-23 \n    2022-01-04 \n    1,422 \n    101,892 \n    408 \n    164 \n    0.014 \n    3.5 \n    8.7 \n    1.6 \n  \n  \n    Factual V3 \n    2021-12-23 \n    2022-01-04 \n    1,415 \n    112,041 \n    496 \n    174 \n    0.013 \n    2.9 \n    8.1 \n    1.6 \n  \n  \n    Branded \n    2022-01-07 \n    2022-01-17 \n    1,022 \n    51,911 \n    206 \n    65 \n    0.020 \n    5.0 \n    15.7 \n    1.3 \n  \n  \n    PPCo Creatives \n    2022-01-07 \n    2022-01-17 \n    1,027 \n    79,241 \n    327 \n    106 \n    0.013 \n    3.1 \n    9.7 \n    1.3 \n  \n  \n    PPCo Creatives \n    2022-01-07 \n    2022-01-18 \n    366 \n    24,106 \n    96 \n    30 \n    0.015 \n    3.8 \n    12.2 \n    1.2 \n  \n\n\n\na 'False start' campaign dates with less than 200 reach are excluded\n\n\n\n\n\n\n\nThe results varied substantially by campaign, but this could be attributed to a range of factors, including different sets of videos and texts in each campaign, different versions of videos presented on these dates, and changes in audience filters. As these campaigns were administered on different dates, there may also be uncontrolled differences in the population seeing our ads.\n\n\nOutcomes for the most comparable trials and A/B tests\nThe campaigns run on the same dates are the most comparable, and in some cases, were explicitly set up as A/B tests.\n\n\n\n\n\n\nA/B trial setups\n\n\n\n\n\nTest 1 (Nov 30, 2021 – Dec 8, 2021) displayed either the long factual video or a cause focus video. In the cause focus condition, cause-specific audiences for animal rights, climate change, and poverty (based on their behavior on Facebook) were shown the relevant cause video.\n(Note: the ‘Only 3% research’ and ‘100x impact’ messages were not shown )\nTest 2 (Dec 8 - 20, 2021) was the same as Test 1 but used the short factual video instead of the cause-focus videos.\nTest 3 (Dec 23, 2021 - Jan 4, 2022) was the same as Test 2 but had a new version of the videos (with Luke just holding up signs with the words). This test was also restricted to 18-35 year olds.\nTest 4: The brand video was displayed in a separate branded campaign which was tested against another campaign that allowed the algorithm to optimize between the short factual and cause-focus videos (although not allowing each cause-specific audience to see the ads for other cause areas).\n\n\n\nAs noted, some campaigns were set up explicitly as A/B trials. Below, we focus specifically on the comparable groups in each.11\n\n\nCode\n(\n  campaign_date_outcomes_comp_1_2 <-  gg_campaign_by_ad_by_text_age_gender %>%\n    filter(starts<= \"2021-12-08\") %>% \n    filter(!str_det(text_treat, \"100x impact|Overwhelming|Only 3% research|Cause list\")) %>%\n    filter(audience == \"Philanthropy\" | audience ==\"Lookalikes\") %>%\n    group_by(video_theme, audience) %>%\n    filter(reach>100) %>%\n    base_results_sum %>%\n     dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    arrange(audience, -`Results/ 1k impr.`) %>% \n    .kable(caption = \"'Comparable' Parts of Tests 1 & 2: reach, impressions, clicks, results\") %>%\n    .kable_styling() \n)\n\n\n\n\n'Comparable' Parts of Tests 1 & 2: reach, impressions, clicks, results\n \n  \n    video_theme \n    audience \n    reach \n    $/ impr. \n    $/ click \n    $/ result \n    Results/ 1k impr. \n  \n \n\n  \n    Factual short \n    Lookalikes \n    5,415 \n    0.038 \n    2.5 \n    7.4 \n    5.2 \n  \n  \n    Cause-led (any) \n    Lookalikes \n    3,332 \n    0.046 \n    4.6 \n    9.5 \n    4.8 \n  \n  \n    Factual short \n    Philanthropy \n    6,716 \n    0.040 \n    3.1 \n    9.0 \n    4.5 \n  \n  \n    Cause-led (any) \n    Philanthropy \n    42,459 \n    0.041 \n    4.3 \n    12.4 \n    3.3 \n  \n  \n    Factual long \n    Philanthropy \n    3,532 \n    0.054 \n    9.0 \n    31.7 \n    1.7 \n  \n\n\n\n\n\nThe table above is limited to the campaigns starting on or before 2021-12-08: the ‘tests 1 and 2’. It is limited to Philanthropy and Lookalike audiences, the only audiences that saw all videos. It is limited to text (message) treatments that were shown across all these campaigns (“6000+ people”, “Bigger difference”, and “Learn”). Note the extremely poor performance of the ‘Factual long’ message. The Factual short message slightly outperformed the Cause-led messages overall.12 The Lookalike audience somewhat overperformed the Philanthropy audience, particularly with the cause-led videos.\nNext we consider “Test 3”, starting on 2021-12-23. As noted, this had a new version of the videos, and new age restrictions\n\n\nCode\n(\n  campaign_date_outcomes_comp_3 <-  gg_campaign_by_ad_by_text_age_gender %>%\n    filter(starts== \"2021-12-23\") %>% \n    filter(!str_det(text_treat, \"100x impact|Overwhelming|Only 3% research|Cause list\")) %>%\n    filter(audience == \"Philanthropy\" | audience ==\"Lookalikes\") %>%\n    group_by(video_theme, audience) %>%\n    filter(reach>100) %>%\n    base_results_sum %>%\n     dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    arrange(audience, -`Results/ 1k impr.`) %>% \n    .kable(caption = \"'Comparable' Parts of Tests 3\") %>%\n    .kable_styling() \n)\n\n\n\n\n'Comparable' Parts of Tests 3\n \n  \n    video_theme \n    audience \n    reach \n    $/ impr. \n    $/ click \n    $/ result \n    Results/ 1k impr. \n  \n \n\n  \n    Cause-led (any) \n    Lookalikes \n    5,758 \n    0.018 \n    2.5 \n    10.5 \n    1.7 \n  \n  \n    Factual short \n    Lookalikes \n    8,609 \n    0.016 \n    3.9 \n    9.3 \n    1.7 \n  \n  \n    Cause-led (any) \n    Philanthropy \n    28,074 \n    0.013 \n    4.2 \n    9.4 \n    1.4 \n  \n  \n    Factual short \n    Philanthropy \n    13,832 \n    0.012 \n    3.4 \n    10.8 \n    1.1 \n  \n\n\n\n\n\nThis has the same filters as the previous table: only Philanthropy and Lookalike audiences13 and only those text treatments shown to all. Here, at first pass, both audiences, and both sets of video treatment themes seem to perform roughly the same on a results-per-cost basis.14\n\nFocus: Test 4, videos and audiences\nFinally, we focus on test 4, which incorporated the branded video. Here all text treatments could be paired with each of the videos, and each were given to each audience. Thus, we can ‘include a lot’ and this table is large (thus the ‘datatables’ format, allowing sorting and filtering).15\n\n\nCode\ncampaign_date_outcomes_comp_4_vid <-  gg_video_breakdowns %>%\n    filter(starts== \"2022-01-07\") %>% \n    group_by(video_theme, audience) %>%\n    filter(audience!=\"Retargeting\") %>%\n    base_results_sum %>%\n     dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    arrange(audience, `$/ result`) %>% \n    dplyr::select(audience, everything())\n\ncampaign_date_outcomes_comp_4_vid %>% \n        DT::datatable(caption = \"Test 4 (roughly comparable); blank cells indicate NA/no results\", filter=\"top\",  rownames= FALSE) \n\n\n\n\n\n\n\n\n\nWithin each audience16\nFor the Philanthropy audience, the Climate and Poverty videos and the Factual Short videos did about equally well – about 8-9 USD per result. The Branded video did slightly worse. The Animal video performed very poorly on this audience!\nFor the General audience, the Climate video did best, achieving a result at close to 6 dollars. The Animal and Factual videos did somewhat worse, and the Poverty and Brand videos did substantially worse on a results per cost basis.\n\n\n\n\n\n\nFormat note\n\n\n\n\n\nThe table above is presented with the Datatables package/function. This allows sorting, filtering, etc. We can present more tables in this format if it is preferable.\n\n\n\n\n\nFactual short versus Branded videos\nThe cost per result for the Factual Short video was fairly constant across audiences (except the Lookalikes, who performed very poorly with this video). The Branded video performed adequately on the Lookalikes and Philanthropy audiences, but extremely poorly on the General and cause audiences.\n\n\nCause videos\n\nAnimals video: did approximately equally well (about 2 results per 1k impressions and 9-11 dollars per result) on all audiences except the Philanthropy audience.\nClimate video: Surprisingly poor performance for the climate audience (but performed well with theGeneral audience, and OK with the Philanthropy and Lookalike audiences\nPoverty video: This video apparently had poor performance overall, leading FB’s algorithm to largely drop it. It seems to have not appealed to any audience except the Philanthropy audience, where it did marginally OK.\n\n\n\nSorting by ‘$/results overall’\nThe best audience-video combinations were:\n\n\nCode\ncampaign_date_outcomes_comp_4_vid %>% filter(`$/ result`<=9.1) %>% dplyr::select(audience, video_theme, `$/ result`) %>% \n  dplyr::arrange(`$/ result`) %>% \n  .kable() %>% .kable_styling()\n\n\n\n\n \n  \n    audience \n    video_theme \n    $/ result \n  \n \n\n  \n    General audience \n    Climate \n    6.2 \n  \n  \n    Global Poverty \n    Factual short \n    7.8 \n  \n  \n    Philanthropy \n    Climate \n    8.4 \n  \n  \n    Lookalikes \n    Climate \n    9.0 \n  \n  \n    Philanthropy \n    Poverty \n    9.0 \n  \n\n\n\n\n\n… each of which cost at or below 9 dollars per result\nThe worst audience-video combinations were:\n\n\nCode\ncampaign_date_outcomes_comp_4_vid %>% filter(`$/ result`>20 |  is.na(`$/ result`) ) %>% dplyr::select(audience, video_theme, `$/ result`) %>% \n    dplyr::arrange(-`$/ result`) %>% \n.kable() %>% .kable_styling()\n\n\n\n\n \n  \n    audience \n    video_theme \n    $/ result \n  \n \n\n  \n    Global Poverty \n    Poverty \n    Inf \n  \n  \n    Lookalikes \n    Poverty \n    Inf \n  \n  \n    Lookalikes \n    Factual short \n    Inf \n  \n  \n    Philanthropy \n    Animal \n    36.5 \n  \n  \n    Animal \n    Branded (factual) \n    24.8 \n  \n  \n    Global Poverty \n    Branded (factual) \n    23.3 \n  \n  \n    Climate \n    Branded (factual) \n    20.7 \n  \n\n\n\n\n\n… each of which cost over 20 dollars per result (or had no results, suggesting even higher costs).17\nComparing audiences in Test 4\n\n\n\nHere the General and Philanthropy audiences performed about equally well, while the cause audiences performed somewhat worse, particularly the Poverty audience.\n\n\n\nCode\ncampaign_date_outcomes_comp_4_aud <-  gg_video_breakdowns %>%\n    filter(starts== \"2022-01-07\") %>% \n      group_by(audience) %>%\n    filter(audience!=\"Retargeting\") %>%\n    base_results_sum %>%\n    mutate(`Video type` = \"All\") %>% \n     dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n     dplyr::select(audience, `Video type`, everything()) %>% \n      arrange(`Video type`, `$/ result`)\n\n\n\ncampaign_date_outcomes_comp_4_aud_vt <-  gg_video_breakdowns %>%\n    mutate(`Video type` = if_else(str_det(video_theme, \"Factual|factual\"), \"Non-cause\", \"Cause\")) %>% \n    filter(starts== \"2022-01-07\") %>% \n      group_by(`Video type`, audience) %>%\n    filter(audience!=\"Retargeting\") %>%\n    base_results_sum %>%\n     dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    arrange(`Video type`, `$/ result`) %>% \n    dplyr::select(audience, everything())\n\ncampaign_date_outcomes_comp_4_aud_vt_all <- bind_rows(campaign_date_outcomes_comp_4_aud, campaign_date_outcomes_comp_4_aud_vt)\n\ncampaign_date_outcomes_comp_4_aud_vt_all %>% \n        DT::datatable(caption = \"Test 4 by audience and video type\", filter=\"top\",  rownames= FALSE) \n\n\n\n\n\n\n\n\n\n\n\n\nFocus: Test 4, Texts and audiences\n\n\nCode\ncampaign_date_outcomes_comp_4_text_pool <-  gg_campaign_by_ad_by_text_age_gender %>%\n    filter(starts== \"2022-01-07\") %>% \n    group_by(text_treat) %>%\n    filter(audience!=\"Retargeting\") %>%\n    base_results_sum %>%\n     dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    arrange(`$/ result`) \n\ncampaign_date_outcomes_comp_4_text_pool %>% \n        DT::datatable(caption = \"Tests 4: Performance by text; blank cells indicate NA/no results\") \n\n\n\n\n\n\n\nCode\nworst_to_best <- campaign_date_outcomes_comp_4_text_pool[[5,5]]/campaign_date_outcomes_comp_4_text_pool[[1,5]]\n\n\nAbove, pooling across all audiences (except Retargeting) and weighted towards ‘those people in each audience who got each message’, we see the “Only 3% research” message performed best per dollar, closely followed by the “100x impact” and “Bigger difference” messages. “Learn” did substantially worse, and “6000+ people” the worst, costing 1.84 times as much as the best message.\n\n\nNext, we consider these messages by audience, for this trial.\n\n\n\nCode\ncampaign_date_outcomes_comp_4_text <-  gg_campaign_by_ad_by_text_age_gender %>%\n    filter(starts== \"2022-01-07\") %>% \n    group_by(text_treat, audience) %>%\n    filter(audience!=\"Retargeting\") %>%\n    base_results_sum %>%\n     dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    arrange(audience, `$/ result`) %>% \n    dplyr::select(audience, everything())\n\ncampaign_date_outcomes_comp_4_text %>% \n        DT::datatable(caption = \"Tests 4: Performance by audience and text; blank cells indicate NA/no results\", filter=\"top\",  rownames= FALSE) \n\n\n\n\n\n\n\n\nSome messages show strong heterogeneity across audiences, while others were fairly consistent. The “Only 3%” message does well on the Animal and Philanthropy audiences, OK on the General audience, but does very poorly on the other audiences. The second best overall message, “100x impact” does so/so on all audiences. “Learn” shows some variation, doing pretty well on Poverty and Lookalike audiences, OK on Animal audiences, but poorly on the General, Philanthropy and Climate audiences. The “6000+ people” and “Bigger Difference” messages perform poorly on nearly all audiences, doing at best-OK on a few audiences. (Use the above ‘filter’ functions to make these comparisons clear.)\n\n\nOther ‘outcome by group’ comparisons across several trials\nNext, we show the results by age and gender. As our age filters changed over time, we do this first for the earlier trials, when all age groups were included.\n\n\nCode\n(\n  age_outcomes_pre_12_09 <- gg_campaign_by_ad_by_text_age_gender %>%\n    group_by(age) %>%\n        filter(reach>500, starts<= \"2021-12-08\") %>%\n    base_results_sum() %>%\n    .kable(caption = \"Results by Age: Campaigns starting on or before Dec 8 2021\") %>%\n    .kable_styling()\n)\n\n\n\n\nResults by Age: Campaigns starting on or before Dec 8 2021\n \n  \n    age \n    Cost \n    reach \n    Link clicks \n    Results \n    $/ impr. \n    $/ click \n    $/ result \n    Results/ 1k impr. \n  \n \n\n  \n    18-24 \n    275 \n    14,690 \n    48 \n    20 \n    0.019 \n    5.7 \n    13.8 \n    1.4 \n  \n  \n    25-34 \n    585 \n    26,970 \n    162 \n    75 \n    0.022 \n    3.6 \n    7.8 \n    2.8 \n  \n  \n    35-44 \n    625 \n    22,876 \n    156 \n    64 \n    0.027 \n    4.0 \n    9.8 \n    2.8 \n  \n  \n    45-54 \n    970 \n    25,525 \n    246 \n    98 \n    0.038 \n    3.9 \n    9.9 \n    3.8 \n  \n  \n    55-64 \n    1,625 \n    36,157 \n    475 \n    150 \n    0.045 \n    3.4 \n    10.8 \n    4.1 \n  \n  \n    65+ \n    2,758 \n    43,547 \n    1,097 \n    319 \n    0.063 \n    2.5 \n    8.6 \n    7.3 \n  \n\n\n\n\n\nWhile 18 older age groups yield more results per impression, they are also more expensive. This approximately balances out, although the age 18-24 group is particularly costly per result.\nIn later trials we only targeted the younger age groups; the costs per result were similar to earlier trials , and fairly close among the younger age groups (see fold).\n\n\n\n\n\n\nBy age, later trials\n\n\n\n\n\n\n\nCode\n(\n  age_outcomes_post_12_09 <- gg_campaign_by_ad_by_text_age_gender %>%\n    group_by(age) %>%\n        filter(reach>500, starts > \"2021-12-08\") %>%\n    base_results_sum() %>%\n    .kable(caption = \"Results by Age: Campaigns starting on or after Dec 23 2021\") %>%\n    .kable_styling()\n)\n\n\n\n\nResults by Age: Campaigns starting on or after Dec 23 2021\n \n  \n    age \n    Cost \n    reach \n    Link clicks \n    Results \n    $/ impr. \n    $/ click \n    $/ result \n    Results/ 1k impr. \n  \n \n\n  \n    18-24 \n    802 \n    67,114 \n    221 \n    87 \n    0.012 \n    3.6 \n    9.2 \n    1.3 \n  \n  \n    25-34 \n    2,505 \n    175,595 \n    754 \n    270 \n    0.014 \n    3.3 \n    9.3 \n    1.5 \n  \n  \n    35-44 \n    1,016 \n    63,882 \n    315 \n    108 \n    0.016 \n    3.2 \n    9.4 \n    1.7 \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote on change in table style\n\n\n\n\n\nThe tables above make it clear how the relevant ‘per dollar’ and ’per unique impression results are calculated. In the following tables we present slightly fewer columns, for readability.\n\n\n\nWomen (or those identifying as female) gave their emails at a somewhat higher rate overall, but their (unique) impressions were a bit more costly. Thus, ‘cost per result’ roughly balanced out. Nonetheless, this might be somewhat informative for other contexts; wher costs are equal, women might be a particularly promising audience.\n\n\nCode\n(\n  gender_outcomes <- gg_campaign_by_ad_by_text_age_gender %>%\n    group_by(gender) %>%\n    base_results_sum() %>%\n dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    .kable(caption = \"Results by Gender\") %>%\n    .kable_styling()\n)\n\n\n\n\nResults by Gender\n \n  \n    gender \n    reach \n    $/ impr. \n    $/ click \n    $/ result \n    Results/ 1k impr. \n  \n \n\n  \n    female \n    506,889 \n    0.025 \n    3.4 \n    10.1 \n    2.5 \n  \n  \n    male \n    159,525 \n    0.018 \n    3.5 \n    10.2 \n    1.8 \n  \n  \n    unknown \n    15,642 \n    0.015 \n    2.1 \n    7.1 \n    2.2 \n  \n\n\n\n\n\nNext we describe the outcomes by our video treatments, focusing on the philanthropy-interested audience only, for comparability.\n\n\nCode\nvideo_outcomes_phil_0 <- gg_video_breakdowns %>%\n    filter(audience==\"Philanthropy\") %>%\n    group_by(video_theme) %>%\n    base_results_sum() \n\n(\nvideo_outcomes_phil <- video_outcomes_phil_0 %>% \n     dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    arrange(video_theme) %>%\n    .kable(caption = \"Results by Video theme for 'Philanthropy' audience\") %>%\n    .kable_styling()\n)\n\n\n\n\nResults by Video theme for 'Philanthropy' audience\n \n  \n    video_theme \n    reach \n    $/ impr. \n    $/ click \n    $/ result \n    Results/ 1k impr. \n  \n \n\n  \n    Animal \n    63,347 \n    0.036 \n    4.4 \n    13.5 \n    2.7 \n  \n  \n    Climate \n    11,854 \n    0.023 \n    2.8 \n    7.0 \n    3.2 \n  \n  \n    Poverty \n    58,548 \n    0.018 \n    3.5 \n    9.8 \n    1.8 \n  \n  \n    Factual short \n    51,761 \n    0.024 \n    3.0 \n    9.3 \n    2.6 \n  \n  \n    Factual long \n    5,561 \n    0.055 \n    6.8 \n    27.8 \n    2.0 \n  \n  \n    Branded (factual) \n    11,902 \n    0.024 \n    4.3 \n    11.6 \n    2.1 \n  \n\n\n\n\n\nCode\nbot_table <- function(df,  outvar, outcol) {\n  df %>% \n    dplyr::arrange({{outvar}}) %>%\n    dplyr::select({{outcol}}) %>% \n    mutate_if(is.factor, as.character) %>%\n    .[[1,1]] \n  } \n\ntop_table <- function(df, outcol, outvar) {\n  df %>%\n    dplyr::arrange(-{{outvar}}) %>%\n    dplyr::select({{outcol}}) %>%\n    mutate_if(is.factor, as.character) %>%\n    .[[1,1]]\n}\n\n\n# { ov <- {{outvar}}\n# ifelse(reverse,\n#          dplyr::arrange(ov),\n#    dplyr::arrange(-ov)\n#   )} %>% \n\n#   \n#   dplyr::arrange({{outvar}}),\n #         dplyr::arrange(-{{outvar}}))} %>% \n\ntop_vid_phil <- video_outcomes_phil_0 %>% bot_table( outvar=`$/ result`, outcol=video_theme)\nbot_vid_phil <- video_outcomes_phil_0 %>% top_table( outvar=`$/ result`, outcol=video_theme)\n\n\nThe Climate video performed particularly well on the philanthropy-interested audience, while “Factual long” performed the worst.19\nNext, we compare the text treatments for the later campaigns only. The earlier and later campaigns had a slightly different set of texts; combining across these indeed risks confounding multiple dimensions.\n\n\nCode\noutcomes_by_text <- gg_campaign_by_ad_by_text_age_gender %>%\n    filter(str_det(campaign_name, \"factual|branded\")) %>%\n    group_by(text_treat) %>%\n    base_results_sum() \n\noutcomes_by_text %>%   \n   dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    .kable(caption = \"Results by text; later campaigns\") %>%\n    .kable_styling()\n\n\n\n\nResults by text; later campaigns\n \n  \n    text_treat \n    reach \n    $/ impr. \n    $/ click \n    $/ result \n    Results/ 1k impr. \n  \n \n\n  \n    100x impact \n    87,369 \n    0.025 \n    3.1 \n    9.6 \n    2.6 \n  \n  \n    6000+ people \n    52,165 \n    0.021 \n    3.9 \n    14.7 \n    1.4 \n  \n  \n    Bigger difference \n    68,308 \n    0.023 \n    3.5 \n    10.1 \n    2.3 \n  \n  \n    Learn \n    51,860 \n    0.024 \n    3.8 \n    13.6 \n    1.8 \n  \n  \n    Only 3% research \n    65,786 \n    0.026 \n    2.3 \n    7.0 \n    3.7 \n  \n\n\n\n\n\nCode\ntop_text_later <- outcomes_by_text %>% bot_table( outvar=`$/ result`, outcol=text_treat)\nbot_text_later <- outcomes_by_text %>% top_table( outvar=`$/ result`, outcol=text_treat)\n\n\nThe “Only 3% research” message performed particularly well on a cost-per-result basis, while 6000+ people performed the worst.\n\n\nFinally, we consider results by audience, focusing on the non-cause and cause treatments separately, for comparability.\n\n\n\nCode\naudience_outcomes_all <- gg_video_breakdowns %>%\n         filter(audience!=\"Retargeting\" & audience!=\"General audience\")  %>% #latter filter because they were only in the final trial\n  mutate(`Video type` = \"All\") %>% \n    group_by(`Video type`, audience) %>%\n    base_results_sum %>%\n   dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    arrange(`Video type`, `$/ result`)\n\naudience_outcomes_vt <- gg_video_breakdowns %>%\n         filter(audience!=\"Retargeting\" & audience!=\"General audience\")  %>% #latter filter because they were only in the final trial\n  mutate(`Video type` = if_else(str_det(video_theme, \"Factual|factual\"), \"Non-cause\", \"Cause\")) %>% \n    group_by(`Video type`, audience) %>%\n    base_results_sum %>%\n   dplyr::select(-Cost, -`Results`, -`Link clicks`) %>%\n    arrange(`Video type`, `$/ result`)\n\n\naudience_outcomes_vt_all <- bind_rows(audience_outcomes_all, audience_outcomes_vt)\n\naudience_outcomes_vt_all %>%\n    DT::datatable(caption = \"Results by audience; cause vs non-cause (and overall)\",  filter=\"top\",  rownames= FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant caveat: bias from divergent delivery here\n\n\n\n\n\nThe above table is somewhat misleading because of the divergent delivery issue. Here, the Global Poverty audience seemed to have done well with videos targeting their oewn cause. However, in each campaign Facebook serves videos less to audiences when they perform poorly on these videos. And in the most comparable (4th) campaign, this combination did so poorly that FB seems to have not administered it much, which makes it not count towards the overall total!"
  },
  {
    "objectID": "chapters/gwwc_gg.html#modeling-and-inference-bayesian-logistic-regressions-and-cis",
    "href": "chapters/gwwc_gg.html#modeling-and-inference-bayesian-logistic-regressions-and-cis",
    "title": "1  Giving What We Can: Giving guides",
    "section": "1.5 Modeling and inference: Bayesian Logistic regressions and CIs",
    "text": "1.5 Modeling and inference: Bayesian Logistic regressions and CIs\n\n\n\n\n\n\nSee discussion of this and other modeling and simulation approaches,\n\n\n\n\n\nMoved to separate file modeling_fb_discussion.qmd, publicly hosted here]\n\n\n\n\n\nLoad tidy Bayes packages\nlibrary(pacman)\n#p_load(brms, install=FALSE)\nlibrary(brms)\n#p_load(tidybayes, install=FALSE)\nlibrary(tidybayes) #integrate Bayesian modeling into a 'tidy data + ggplot' workflow\n\n\n\n\nSumming outcomes by breakdowns of interest, for use in Bayesian modeling\nsum_results <- function(df) {\n    df %>%\n     summarise(\n    results = sum(results, na.rm = TRUE),\n    spend = sum(amount_spent_usd, na.rm = TRUE),\n    clicks = sum(link_clicks, na.rm = TRUE),\n    impressions = sum(impressions, na.rm = TRUE),\n    reach = sum(reach, na.rm = TRUE)\n  )\n}\n\n# collapse into the categories of interest\ngg_video_breakdowns_col <- gg_video_breakdowns %>%\n  group_by(video_theme, audience) %>%\n  #group_by(video_theme, audience, starts) %>%\nsum_results\n#%>%   mutate(starts = as.factor(starts))\n#rem: we are just summing outcomes, so no weights are needed here\n\ngg_video_breakdowns_col_mix <- gg_video_breakdowns %>%\n  group_by(video_theme, audience, starts) %>%\n  #group_by(video_theme, audience, starts) %>%\nsum_results\n#%>%   mutate(starts = as.factor(starts))\n#rem: we are just summing outcomes, so no weights are needed here\n\n# collapse into the categories of interest\ngg_campaign_by_text_col <-  gg_campaign_by_ad_by_text_age_gender %>%\n   uncount(weights = .$reach) %>%\n  group_by(text_treat, audience) %>% \nsum_results\n#%>%   mutate(starts = as.factor(starts))\n#rem: we are just summing outcomes, so no weights are needed here\n\ngg_campaign_by_text_only <-  gg_campaign_by_ad_by_text_age_gender %>%\n   uncount(weights = .$reach) %>%\n  group_by(text_treat) %>% \nsum_results\n\ngg_video_breakdowns_col_mix <- gg_video_breakdowns %>%\n  group_by(video_theme, audience, starts) %>%\n  #group_by(video_theme, audience, starts) %>%\nsum_results\n#%>%   mutate(starts = as.factor(starts))\n#rem: we are just summing outcomes, so no weights are needed here\n\n\n\n\n\n\n\n\nExamples of these summed breakdowns, to be used in Bayesian modeling\n\n\n\n\n\n\n\nCode\ngg_video_breakdowns_col %>% \n  mutate(spend = round(spend,2)) %>% \n  DT::datatable()\n\n\n\n\n\n\n\n\n\n\nNext, we compute and set minimally-informative priors for the click-rates model.\n\n\nPriors for click rate models\n#helper function to 'make priors'\nmake_prior_normal <- function(mean, sd, ...) {\n  prior_string(paste0(\"normal(\", mean, \",\", sd, \")\"), ...)\n}\n\nprior_click_per_reach <- 0.028 #based on outside/prior data; see discussion below\n\nprior_intercept_click <-  logit(prior_click_per_reach)\n\nprior_intercept_click_ub <- 0.15 #a 15% click rate seemed the highest conceivable\n\nse_prior_click <- (logit(prior_intercept_click_ub) - prior_intercept_click)/1.96 #see discussion below\n\nprior_slope_click_ub <- 0.25 #the strongest reasonably-likely impact of a particular add is to increase click rates by 25 percentage points \n\nse_prior_slope_click <- (logit(prior_slope_click_ub) - prior_intercept_click)/1.96\n\nprior_click <- c(\n  make_prior_normal(prior_intercept_click, se_prior_click, class = \"Intercept\"),\nmake_prior_normal(0, se_prior_slope_click, class = \"b\")\n)\n\n#DR @Jamie: This is a log ratio of probabilities, iirc. Why are we assuming it is normally distributed? \n\n\n\n\n\n\n\n\nDetermining appropriate priors for click rates - intercept\n\n\n\n\n\nIn the code chunk just above, we define the priors which we use below.\nWhere did this come from? It is somewhat ad-hoc, but it is unlikely to matter much. Given our very large dataset, we don’t expect our results to be very sensitive to the priors.\nA 2.8% click rate (as a share of ‘reach’) seemed like a reasonable mean. This is approximately the rate GWWC saw in all of its trials before February 2021, when this trial started. logit(prior_click_per_reach) gives us the intercept associated with this prior expected outcome rate.\nThe prior on the standard error for the intercept is not based on any previous data. We considered ‘what baseline rate would we consider to be extremely unlikely?’, and set the standard deviations to be about half of this (recalling that 95% of the area of a normal distribution is within 1.96 sd’s of the mean). In the positive direction, a rate of 15% or more would be very surprising. This would yield a logit intercept of logit(prior_intercept_click_ub) = -1.7346011. Differencing this from the prior mean and dividing by two yields our chosen prior intercept standard error: 0.925; note this gives a 95 percent lower-bound of a 0.468% click rate.\n\n\n\n\n\n\n\n\n\nDetermining appropriate priors for click rates - coefficients\n\n\n\n\n\nIn determining priors for the slopes (adjustments for different groups), we start with a mean expected slope of 0; this is consistent with a sort of ‘unbiased’ prior, not loading the dice in either direction, and also most adaptable to a range of groups we might consider. We again considered ‘what mean click rates would be very surprising’. A click rate of 25% of more for any targeted subgroup or treatment would be very unexpected – this seems conservative. By similar calculations above, this yields se_prior_slope_click = 1.2492546.\nCould we instead do this considering reasonable ‘proportional differences in rates’? We will consider this for future work (possible challenge: it may interact with the intercepts).\n\n\n\nNext, we compute and set minimally informative priors for the results model.\n\n\nPriors: results model\nprior_results_per_click <- 0.20\n\nprior_intercept_results <-  logit(prior_results_per_click)\n\nprior_intercept_results_ub <- 0.75\nse_prior_results <- (logit(prior_intercept_results_ub) - prior_intercept_results)/1.96\n\nprior_slope_results_ub <- 0.85\nse_prior_slope_results <- (logit(prior_slope_results_ub) - prior_intercept_results)/1.96\n\nprior_results <- c(\n  make_prior_normal(prior_intercept_results, se_prior_results, class = \"Intercept\"),\nmake_prior_normal(0, se_prior_slope_results, class = \"b\")\n)\n\n\n\n\n\n\n\n\nDetermining appropriate priors for results (per click)\n\n\n\n\n\nSee the above discussion folds on priors for clicks for our general approach.\nFor results per click (RpC), we had very little data or experience to go on. The little prior data GWWC had on results per click were from very different contexts, e.g., RSVPs for attending events rather than simply leaving an email. In those cases results per click are (as we expect, are results per click for Facebook ads are in general). However, here people are clicking on a call to action like ‘Download the Effective Giving Guide’, directing them to a site where they merely need to leave their email to download this guide. Thus, if the clicks are intentional, a much higher rate of ‘result’ seems reasonable. Thus we compromised on a 20% ‘results per click’ rate as our overall prior mean.\nWe would be ‘very surprised’ (seems 5% likely or less) with an RpC of over 75% overall or 85% for any subgroup – we derive the standard errors of the intercept and slope from these, as for clicks.\n(Note: this implies a 95% CI lower-bound of a 2.04% overall RpC rate.)\n\n\n\n\nEstimating the models\n\n\nCreating formulas for Bayes models (I try to automate)\nclicks_per_reach_video_aud <- as.formula(\"clicks | trials(reach) ~ video_theme + audience + video_theme:audience\") #DR: I guess trials(reach) indicates a particular transformation of the outcome to suit the logistic equation.  video_theme:audience is specifying a type of interaction term\n\n# +  starts\nresults_per_click_video_aud <- as.formula(\"results | trials(clicks) ~ video_theme*audience\") #todo: use same interaction notation or explain\n\n#  For comparison: a one-step model \nresults_per_reach_video_aud <- as.formula(\"results | trials(reach) ~ video_theme + audience + (video_theme|audience)\")\n\n#including messages\nclicks_per_reach_text_aud <- as.formula(\"clicks | trials(reach) ~ text_treat + audience + text_treat:audience\")\n\n# +  starts\nresults_per_click_text_aud <- as.formula(\"results | trials(clicks) ~ text_treat + audience + text_treat:audience\")\n\n\n#  Fuller, mixed model (todo)\n# results_per_reach_video_aud_mix <- as.formula(\"results | trials(reach) ~ (1|starts) + video_theme + audience + (video_theme | audience) + (video_theme:audience)\")\n\n\n\n\nCode\n# label: bayes_estimates_vid\n#| code-summary: \"Estimating models using brm, focus on video treatment dimension\"\n#| output: false\n#| warning: false\n#| message: false\n\n## passing a list\n#arg.list <- list(init = 0, chains = 4, cores = 4, iter = 2500, warmup = 500, backend = \"cmdstanr\", seed = 1010, silent=2,  refresh = 0) \n\n#note 'do.call' was the start of an attempt to automate\n\nclicks_logit_vid <-  do.call(\"brm\", c(list(\n  data = gg_video_breakdowns_col, \n  formula = clicks_per_reach_video_aud,\n                  family = binomial(\"logit\"), \n                  control = list(adapt_delta = 0.99, max_treedepth = 15),\n                   prior = prior_click),\n                  init = 0, chains = 4, cores = 4, iter = 2500, warmup = 500, seed = 1010, silent=2,  refresh = 0,\n  file = \"clicks_logit_vid.rds\", file_refit = \"on_change\"\n  )\n)\n\nresults_logit_vid <-  do.call(\"brm\", c(list(\n  data = gg_video_breakdowns_col, \n  formula = results_per_click_video_aud,\n                  family = binomial(\"logit\"),\n                  control = list(adapt_delta = 0.99, max_treedepth = 15),\n                  prior = prior_results),\n                  init = 0, chains = 4, cores = 4, iter = 2500, warmup = 500, backend = \"cmdstanr\", seed = 1010, silent=2,  refresh = 0,                   \n  list(threads = threading(8)), \n  file = \"results_logit_vid.rds\", file_refit = \"on_change\")\n)\n\nresults_per_reach_logit_vid <-  do.call(\"brm\", c(list(\n  data = gg_video_breakdowns_col, \n  formula = results_per_reach_video_aud,\n                  family = binomial(\"logit\"),\n                  control = list(adapt_delta = 0.99, max_treedepth = 15),\n                  prior = prior_results),\n                  init = 0, chains = 4, cores = 4, iter = 2500, warmup = 500, backend = \"cmdstanr\", seed = 1010, silent=2,  refresh = 0,                   \n  list(threads = threading(8)), \n  file = \"results_per_reach_logit_vid.rds\", file_refit = \"on_change\")\n)\n\n\n\n\nCode\n# label: bayes_estimates_text\n#| code-summary: \"Estimating models using brm, focus on text treatment dimension\"\n#| output: false\n#| warning: false\n\n#for texts:\nclicks_logit_text <-  do.call(\"brm\", c(list(\n  data = gg_campaign_by_text_col, \n  formula = clicks_per_reach_text_aud,\n                  family = binomial(\"logit\"),\n                  control = list(adapt_delta = 0.99, max_treedepth = 15),\n                   prior = prior_click),\n                  init = 0, chains = 4, cores = 4, iter = 2500, warmup = 500, backend = \"cmdstanr\", seed = 1010, silent=2,  refresh = 0,                   \n   list(threads = threading(8)), \n  file = \"clicks_logit_text.rds\", file_refit = \"on_change\")\n)\n\nresults_logit_text <-  do.call(\"brm\", c(list(\n  data = gg_campaign_by_text_col, \n  formula = results_per_click_text_aud,\n                  family = binomial(\"logit\"),\n                  control = list(adapt_delta = 0.99, max_treedepth = 15),\n                  prior = prior_results),\n                  init = 0, chains = 4, cores = 4, iter = 2500, warmup = 500, backend = \"cmdstanr\", seed = 1010, silent=2,  refresh = 0,                   \n   list(threads = threading(8)), \n  file = \"results_logit_text.rds\", file_refit = \"on_change\")\n)\n\n\nRunning MCMC with 4 parallel chains, with 8 thread(s) per chain...\n\nChain 1 finished in 49.7 seconds.\nChain 4 finished in 56.7 seconds.\nChain 3 finished in 65.8 seconds.\nChain 2 finished in 66.8 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 59.7 seconds.\nTotal execution time: 66.9 seconds.\n\n\nCode\n#for texts only:\nclicks_logit_text_only <-  do.call(\"brm\", c(list(\n  data = gg_campaign_by_text_only, \n  formula = clicks | trials(reach) ~ text_treat,\n                  family = binomial(\"logit\"),\n                  control = list(adapt_delta = 0.99, max_treedepth = 15),\n                   prior = prior_click),\n                  init = 0, chains = 4, cores = 4, iter = 2500, warmup = 500, backend = \"cmdstanr\", seed = 1010, silent=2,  refresh = 0,                   \n   list(threads = threading(8)), \n  file = \"clicks_logit_text_only.rds\", file_refit = \"on_change\")\n)\n\nresults_logit_text_only <-  do.call(\"brm\", c(list(\n  data = gg_campaign_by_text_only, \n  formula = results | trials(clicks) ~ text_treat,\n                  family = binomial(\"logit\"),\n                  control = list(adapt_delta = 0.99, max_treedepth = 15),\n                  prior = prior_results),\n                  init = 0, chains = 4, cores = 4, iter = 2500, warmup = 500, backend = \"cmdstanr\", seed = 1010, silent=2,  refresh = 0,                   \n   list(threads = threading(8)), \n  file = \"results_logit_text_only.rds\", file_refit = \"on_change\")\n)\n\n\nRunning MCMC with 4 parallel chains, with 8 thread(s) per chain...\n\nChain 1 finished in 0.2 seconds.\nChain 2 finished in 0.3 seconds.\nChain 3 finished in 0.3 seconds.\nChain 4 finished in 0.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.3 seconds.\nTotal execution time: 0.4 seconds.\n\n\n\n\n\n\nForest plots\n\n\nfull crossing, posterior expectations\n# posterior expectations:\nfull_crossing <- expand_grid(\n  \"video_theme\" = unique(gg_video_breakdowns_col$video_theme),\n  \"audience\" = unique(gg_video_breakdowns_col$audience)) %>% \n  mutate(reach = 1, #it is estimating the number of clicks based on the reach\n         clicks = 1) %>% \nfilter(!(audience==\"Animal\" & (video_theme == \"Climate\" | video_theme == \"Poverty\")))  %>%\n    filter(!(audience==\"Climate\" & (video_theme == \"Animal\" | video_theme == \"Poverty\")))  %>%\n    filter(!(audience==\"Global Poverty\" & (video_theme == \"Animal\" | video_theme == \"Climate\"))) #remove combinations where we don't have data ... at least for now\n\n# setting reach and clicks = 1 will give us proportion of conversions (because it makes a prediction for total outcomes per unit )\n\nclick_post <- posterior_epred(clicks_logit_vid, newdata = full_crossing) #ndraws = 1000, \nresults_post <- posterior_epred(results_logit_vid, full_crossing)\n\ncombined_post <- click_post * results_post #DR: multiplying the predicted probabilities of click and conversion here?\n\nresults_per_reach_post <- posterior_epred(results_per_reach_logit_vid, full_crossing)\n\n\n\n\nCode\n#make tibbles of the stuff above, put it together s\npost_tib_clean <- function(df, name) {\n  data <- df %>% \n    as_tibble() %>% \n    pivot_longer(cols = everything(),\n    names_to = \"identifier\",\n    values_to = \"probability\") \n  data <- data %>% \n     mutate(level = name,\n       theme = rep(full_crossing$video_theme, dim(data)[1]/dim(full_crossing)[1]),\n       audience = rep(full_crossing$audience,  dim(data)[1]/dim(full_crossing)[1])\n       #     starts = rep(full_crossing$starts, 1000) #DR: I think I want starts (start date as a factor) in the model, because the audience may be systematically different on those days, and other things change we leave out here. However, I don't want to see it in the graphs. How to do that? \n     )\n  return(data)\n}\n\n# make tibbles\ntib_click_post <- click_post %>% \n  post_tib_clean( \"1. Reach to clicks\")\ntib_result_post <- results_post  %>%\n    post_tib_clean(\"2. Clicks to signups\")\ntib_combined_post <- combined_post %>%\n      post_tib_clean(\"3. Total\") \nfull_post <- bind_rows(tib_click_post,\n                       tib_result_post,\n                       tib_combined_post)\n\n\n\n\nCode\npost_tib_clean_text <- function(df, name) {\n  data <- df %>% \n    as.tibble() %>% \n    pivot_longer(cols = everything(),\n    names_to = \"identifier\",\n    values_to = \"probability\") \n  data <- data %>% \n     mutate(level = name)\n       #     starts = rep(full_crossing$starts, 1000) #DR: I think I want starts (start date as a factor) in the model, because the audience may be systematically different on those days, and other things change we leave out here. However, I don't want to see it in the graphs. How to do that? \n  return(data)\n}\n\n\n# posterior expectations:\n\nfull_crossing_text <- expand_grid(\n  \"text_treat\" = unique(gg_campaign_by_text_col$text_treat),\n  \"audience\" = unique(gg_campaign_by_text_col$audience)) %>% \n  mutate(reach = 1, #it is estimating the number of clicks based on the reach\n         clicks = 1) \n\nfull_crossing_text_only <- expand_grid(\n  \"text_treat\" = unique(gg_campaign_by_text_col$text_treat)) %>% \n  mutate(reach = 1, #it is estimating the number of clicks based on the reach\n         clicks = 1)\n\nclick_post_text <- posterior_epred(clicks_logit_text, newdata = full_crossing_text) #ndraws = 1000, \nresults_post_text <- posterior_epred(results_logit_text, full_crossing_text)\ncombined_post_text <- click_post_text * results_post_text \n\ntext_audience_reps <- function(df) {\n  df %>% \n mutate(\n         text_treat = rep(full_crossing_text$text_treat, dim(tib_click_post_text)[1]/dim(full_crossing_text)[1]),\n       audience = rep(full_crossing_text$audience,  dim(tib_click_post_text)[1]/dim(full_crossing_text)[1])\n)}\n  \n\n# make tibbles\ntib_click_post_text <- click_post_text %>% \n  post_tib_clean_text( \"1. Reach to clicks\") \ntib_click_post_text %<>%  text_audience_reps\n\ntib_result_post_text <- results_post_text  %>%\n    post_tib_clean_text(\"2. Clicks to signups\")\ntib_result_post_text %<>% text_audience_reps\n\ntib_combined_post_text <- combined_post_text %>%\n      post_tib_clean_text(\"3. Total\") \ntib_combined_post_text %<>% text_audience_reps\n\nfull_post_text <- bind_rows(tib_click_post_text,\n                       tib_result_post_text,\n                       tib_combined_post_text)\n\n#for text only, 1 variable model\n\nclick_post_text_only <- posterior_epred(clicks_logit_text_only, newdata = full_crossing_text_only) #ndraws = 1000, \n\nresults_post_text_only <- posterior_epred(results_logit_text_only, full_crossing_text_only)\n\ncombined_post_text_only <- click_post_text_only * results_post_text_only\n\n\n# make tibbles\n\ntext_only_reps <- function(df) {\n  df %>% \n   mutate(text_treat = rep(full_crossing_text_only$text_treat, dim(tib_click_post_text_only)[1]/dim(full_crossing_text_only)[1]))\n}          \n\ntib_click_post_text_only <- click_post_text_only %>% \n  post_tib_clean_text( \"1. Reach to clicks\")\ntib_click_post_text_only %<>%   text_only_reps\n\ntib_results_post_text_only <- results_post_text_only  %>%\n    post_tib_clean_text(\"2. Clicks to signups\")\ntib_results_post_text_only %<>%  text_only_reps\n\ntib_combined_post_text_only <- combined_post_text_only %>%\n      post_tib_clean_text(\"3. Total\") \n\ntib_combined_post_text_only %<>%  text_only_reps\n\nfull_post_text_only <- bind_rows(tib_click_post_text_only,\n                       tib_results_post_text_only,\n                       tib_combined_post_text_only)\n\n\n\n\nCode\nhdi <- HDInterval::hdi\n\nCI_choice_narrow <- 0.6\nCI_choice_wide <- 0.9\n\nsum_mean_hdi <- function(\n    df, \n  var = probability, scaleme=100, CI_choice_n=CI_choice_narrow, CI_choice_w = CI_choice_wide) {\n  df %>% \n    summarise(\n      mean = mean({{var}}) * scaleme,\n            lower_n = hdi({{var}}, credMass = CI_choice_n)[1] * scaleme,\n            upper_n = hdi({{var}}, credMass = CI_choice_n)[2] * scaleme,\n              lower_w = hdi({{var}}, credMass = CI_choice_w)[1] * scaleme,\n            upper_w = hdi({{var}}, credMass = CI_choice_w)[2] * scaleme,\n          lower_eti = quantile({{var}}, (1-CI_choice_w)/2) * scaleme,\n    upper_eti = quantile({{var}}, 1-(1-CI_choice_w)/2) * scaleme,\n      check = length(hdi({{var}}))\n    )\n} \n\n\nmutate_mean_hdi <- function(\n    df, \n  var = probability, scaleme=100, CI_choice_n=CI_choice_narrow, CI_choice_w = CI_choice_wide) {\n  df %>% \n    dplyr::mutate(\n      mean = mean({{var}}) * scaleme,\n            lower_n = hdi({{var}}, credMass = CI_choice_n)[1] * scaleme,\n            upper_n = hdi({{var}}, credMass = CI_choice_n)[2] * scaleme,\n              lower_w = hdi({{var}}, credMass = CI_choice_wide)[1] * scaleme,\n            upper_w = hdi({{var}}, credMass = CI_choice_wide)[2] * scaleme,\n          lower_eti = quantile({{var}}, (1-CI_choice_wide)/2) * scaleme,\n    upper_eti = quantile({{var}}, 1-(1-CI_choice_wide)/2) * scaleme,\n      check = length(hdi({{var}}))\n    )\n} \n\n\n\nResults by audience\nWe ran two Bayesian Logit models. 20 Results come from a two-stage process: 1. some people who see the ad click on it. 2. Some of those who click on the ad leave their email (asking for a Giving Guide). We model clicks as a share of unique impressions (‘reach’) and results as a share of clicks, allowing each to vary by video theme and by audience. (Later: by text and by demographics.) The product of these shares (probabilities) yields ‘results as a share of impressions’, the main outcome of interest.\nBelow, we plot the point means (aka ‘coefficients’) and ‘highest density intervals’ (HDI) of our posterior beliefs for these. These models consider the audience and video themes at the same time, the imbalance between audiences and themes should probably not be a major biasing factor.\nCaveats:\n\nThe specific coefficients for the second stage (‘clicks to results’) should be taken lightly; as the audiences may be selected very differently ‘conditional on click’; e.g., audiences that are ‘easy to click’ may by ‘harder to convert to a result’.\nThe results presented below do not control for ‘which text treatment’ nor for ‘which campaign’. (We can do the latter next, but we can only do the former with the other version of the data, which is then missing some detail on who saw which video)\nThe usual ‘divergent delivery’ issue\n\n\n\n\n\n\n\nNote\n\n\n\nImportant note: the ‘pooled’ graphs (for individual audiences and videos) in this section may not be correctly weighted across subcategories. We need to reexamine this.\n\n\n\n\nCode\naud_plots <- function(df) {\n  df %>% \n       filter(audience!=\"Retargeting\")  %>%\n    group_by(level, audience) %>% #, starts\n    sum_mean_hdi %>% \n    mutate(audience = reorder(as.factor(audience), `mean`)) %>% \n    ggplot() + \n  geom_errorbarh(aes(xmin = lower_n, xmax = upper_n, y = audience), height = .7, color = \"red\") +\n  geom_errorbarh(aes(xmin = lower_w, xmax = upper_w, y = audience),  height=.25, color = \"blue\") +\n  geom_point(\n    aes(x=`mean`, \n    y = audience))\n    } \n\n(\nreach_to_clicks_aud_plot <-  full_post  %>%\n    aud_plots() +\n  labs(title = \"By audience (reach, clicks, signups)\",\n       x = \"Estimated % 'converting' with 60% and 90% HDI\")  +\n      facet_grid(~level, scales = \"free\")\n)\n\n\n\n\n\nCode\n(\nreach_to_clicks_aud_plot_no_cause <-  full_post  %>%\n    filter(!str_det(theme, \"Animal|Climate|Poverty\"))  %>%\n        aud_plots() +\n labs(title = \"By audience, no cause videos\",\n       x = \"Estimated % 'converting' with 60% and 90% HDI\")  +\n      facet_grid(~level, scales = \"free\")\n)\n\n\n\n\n\nCode\n(\nreach_to_clicks_aud_plot_cause <-  full_post  %>%\n    filter(str_det(theme, \"Animal|Climate|Poverty\"))  %>%\n        aud_plots() +\n labs(title = \"By audience, Cause videos only\",\n       x = \"Estimated % 'converting' with 60% and 90% HDI\")  +\n      facet_grid(~level, scales = \"free\")\n)\n\n\n\n\n\nFor the ‘cause videos only’ we see 21 that the philanthropy audience, across all causes, seems to perform at least as well as the climate and ‘global poverty’ audiences (when the latter are presented videos for the causes they are said to care about). However, the animal audiences seems to perform substantially better.\nFor example, focusing on climate-cause videos (and removing ‘lookalikes’) below, we see that the philanthropy audience performs substantially better than the climate audience.\n\n\nCode\n(\nreach_to_clicks_aud_plot_climate <-  full_post  %>%\n    filter(str_det(theme, \"Climate\"))  %>%\n    filter(audience!=\"Lookalikes\") %>% \n        aud_plots() +\n labs(title = \"By audience, Climate videos only\",\n       x = \"Estimated % 'converting' with 60% and 90% HDI\")  +\n      facet_grid(~level, scales = \"free\")\n)\n\n\n\n\n\n\n\n\nResults by Video\n22\n\n\nCode\nvid_plots <- function(df) {\n  df %>% \n    group_by(level, theme) %>% #, starts\n    sum_mean_hdi %>% \n    mutate(theme = reorder(as.factor(theme), `mean`)) %>% \n    ggplot() + \n  geom_errorbarh(aes(xmin = lower_n, xmax = upper_n, y = theme), height = .7, color = \"red\") +\n  geom_errorbarh(aes(xmin = lower_w, xmax = upper_w, y = theme),  height=.25, color = \"blue\") +\n  geom_point(\n    aes(x=`mean`, \n    y = theme))\n    } \n\n(\nreach_to_clicks_vid_plot <-  full_post  %>%\n    vid_plots() +\n  labs(title = \"By video theme (reach, clicks, signups)\",\n       x = \"Estimated % 'converting' with 60% and 90% HDI\")  +\n      facet_grid(~level, scales = \"free\")\n)\n\n\n\n\n\nCode\n(\nreach_to_clicks_vid_plot_phil <-  full_post  %>%\n            filter(audience == \"Philanthropy\")  %>%\n    vid_plots() +\n  labs(title = \"By theme, 'philanthropy' audience only\",\n       x = \"Estimated % 'converting' with 60% and 90% HDI\")  +\n      facet_grid(~level, scales = \"free\")\n)\n\n\n\n\n\n\n\nCode\n(\nreach_to_clicks_plot <-  full_post  %>%\n      filter(grepl(\"1\", level)) %>% \n    group_by(theme, audience) %>% #, starts\n    sum_mean_hdi %>% \n      mutate(theme = reorder(as.factor(theme), `mean`)) %>% \n      filter(audience!=\"Retargeting\")  %>%\n    ggplot() + \n  geom_errorbarh(aes(xmin = lower_n, xmax = upper_n, y = theme), height = .7, color = \"red\") +\n  geom_errorbarh(aes(xmin = lower_w, xmax = upper_w, y = theme),  height=.25, color = \"blue\") +\n  geom_point(aes(x = mean, y = theme)) +\n  coord_cartesian(xlim=c(0, 1.75)) +\n  facet_wrap(~audience, scales = \"fixed\") +\n  labs(title = \"Clicks by video theme and audience\",\n       x = \"Estimated % clicking with 60% and 90% HDIs\") +\n  facet_wrap(~audience, scales = \"fixed\")\n)\n\n\n\n\n\nCode\n(\nreach_to_results_plot <-  full_post %>% \n      filter(grepl(\"3\", level)) %>% \n      group_by(theme, audience) %>% #, startslevel, \n      sum_mean_hdi %>% \n        mutate(theme = reorder(as.factor(theme), `mean`)) %>% \n      filter(audience!=\"Retargeting\")  %>%\n    ggplot() + \n  geom_errorbarh(aes(xmin = lower_n, xmax = upper_n, y = theme), height = .7, color = \"red\") +\n  geom_errorbarh(aes(xmin = lower_w, xmax = upper_w, y = theme),  height=.25, color = \"blue\") +  geom_point(aes(x = mean, y = theme)) +\n  facet_wrap(~audience, scales = \"fixed\") +\n    coord_cartesian(xlim=c(0, 0.8)) +\n  labs(title = \"Reach to results (total), by audience by theme\",\n       x = \"Estimated % results with 60% and 90% HDIs\") +\n  facet_wrap(~factor(audience, levels=audience_levels), scales = \"fixed\")\n)\n\n\n\n\n\nCode\n(\nreach_to_results_plot_flip <-  full_post %>% \n      filter(grepl(\"3\", level)) %>% \n      group_by(audience, theme) %>% #, startslevel, \n      sum_mean_hdi %>% \n        mutate(audience = reorder(as.factor(audience), `mean`)) %>% \n      filter(audience!=\"Retargeting\")  %>%\n    ggplot() + \n  geom_errorbarh(aes(xmin = lower_n, xmax = upper_n, y = audience), height = .7, color = \"red\") +\n  geom_errorbarh(aes(xmin = lower_w, xmax = upper_w, y = audience),  height=.25, color = \"blue\") +  geom_point(aes(x = mean, y = audience)) +\n  facet_wrap(~audience, scales = \"fixed\") +\n    coord_cartesian(xlim=c(0, 0.8)) +\n  labs(title = \"Reach to results (total), by theme by audience\",\n       x = \"Estimated % results with 60% and 90% HDIs\") +\n  facet_wrap(~theme, scales = \"fixed\")\n)\n\n\n\n\n\n\n\nResults by text\n\n\nCode\nreach_to_results_sum_text_only <-  full_post_text_only %>% \n      filter(grepl(\"3\", level)) %>% \n      group_by(text_treat) %>% #, startslevel, conte\n      sum_mean_hdi( CI_choice_n=0.90, CI_choice_w = 0.99) %>%         \n  mutate(text_treat = reorder(as.factor(text_treat), `mean`)) \n\nreach_to_results_sum_text_only  %>%\n  DT::datatable(caption = \"Texts: mean performance by text, 0% and 99% HDI bounds\", filter=\"top\",  rownames= FALSE) %>%\n  formatRound(1:length(reach_to_results_sum_text_only), digits=3)\n\n\n\n\n\n\n\nCode\n(\nreach_to_results_sum_text_only_plot <- \nreach_to_results_sum_text_only %>% \n    ggplot() + \n  geom_errorbarh(aes(xmin = lower_n, xmax = upper_n, y = text_treat), height = .7, color = \"red\") +\n  geom_errorbarh(aes(xmin = lower_w, xmax = upper_w, y = text_treat),  height=.25, color = \"blue\") +  geom_point(aes(x = mean, y = text_treat)) +\n    coord_cartesian(xlim=c(0.13, .45)) +\n  labs(title = \"Reach to results (total), by text\",\n       x = \"Estimated % results with 90% and 99% HDIs\") \n)\n\n\n\n\n\n\n\nCode\nreach_to_results_sum_text <-  full_post_text %>% \n      filter(grepl(\"3\", level)) %>% \n      group_by(text_treat, audience) %>% #, startslevel, \n      sum_mean_hdi %>%         \n  mutate(text_treat = reorder(as.factor(text_treat), `mean`)) %>% \n      filter(audience!=\"Retargeting\")  \n\nreach_to_results_sum_text  %>%\n  DT::datatable(caption = \"Texts:  mean performance by audience, 60 and 90% HDI bounds\", filter=\"top\",  rownames= FALSE) %>%  \n    formatRound(1:length(reach_to_results_sum_text_only), digits=3)\n\n\n\n\n\n\n\nCode\n(\nreach_to_results_sum_text_plot <- \nreach_to_results_sum_text %>% \n    ggplot() + \n  geom_errorbarh(aes(xmin = lower_n, xmax = upper_n, y = text_treat), height = .7, color = \"red\") +\n  geom_errorbarh(aes(xmin = lower_w, xmax = upper_w, y = text_treat),  height=.25, color = \"blue\") +  geom_point(aes(x = mean, y = text_treat)) +\n  facet_wrap(~audience, scales = \"fixed\") +\n    coord_cartesian(xlim=c(0, .6)) +\n  labs(title = \"Reach to results (total), by audience by text\",\n       x = \"Estimated % results with 60% and 90% HDIs\") +\n  facet_wrap(~factor(audience, levels=audience_levels), scales = \"fixed\") \n)\n\n\n\n\n\nCode\n(\nreach_to_results_plot_flip <-  full_post_text %>% \n      filter(grepl(\"3\", level)) %>% \n      group_by(audience, text_treat) %>% #, startslevel, \n      sum_mean_hdi %>% \n        mutate(audience = reorder(as.factor(audience), `mean`)) %>% \n      filter(audience!=\"Retargeting\")  %>%\n    ggplot() + \n  geom_errorbarh(aes(xmin = lower_n, xmax = upper_n, y = audience), height = .7, color = \"red\") +\n  geom_errorbarh(aes(xmin = lower_w, xmax = upper_w, y = audience),  height=.25, color = \"blue\") +  geom_point(aes(x = mean, y = audience)) +\n  facet_wrap(~audience, scales = \"fixed\") +\n    coord_cartesian(xlim=c(0, 1)) +\n  labs(title = \"Reach to results (total), by text by audience\",\n       x = \"Estimated % results with 60% and 90% HDIs\") +\n  facet_wrap(~text_treat, scales = \"fixed\")\n)\n\n\n\n\n\n\n\nOutcomes by cost\nNote: We don’t report on the ‘text’ treatment in this section (for now), because the cost per unique impression was very similar across these texts.\n\n\nCode\n# Joining spending and conversion ####\n\ncost_tibble <- \n  left_join(tib_combined_post,\n    rename(gg_video_breakdowns_col, theme=video_theme)) %>% \n  mutate(\n    reach_per_dollar = reach / spend,\n    sign_per_dollar = reach_per_dollar * probability,\n    #NOTE this uses the simulated distribution of probabilities, not just averages! \n    sign_per_100d = 100*sign_per_dollar,\n    cost_per_signup = 1/sign_per_dollar\n    ) %>% \n  filter(is.na(spend) == FALSE)  \n\ncost_summary <- cost_tibble %>% \n  group_by(theme, audience) %>% \n  sum_mean_hdi(var= sign_per_100d, scaleme=1)\n\n#dim(filter(cost_summary, check > 2))[1] ... where HDI is not continuous, I guess... this doesn't happen here atm\n\n\n\n\nCode\n(\n  sign_per_100d_plot_vid_by_aud <- cost_tibble %>%\n      filter(audience!=\"Retargeting\")  %>%\nggplot() +\n  scale_x_continuous(limits = c(0, 20)) +\n  ggridges::geom_density_ridges(\n    aes(\n      x = sign_per_100d, \n      y = theme\n      )\n    ) +\n  geom_point(data = cost_summary %>% filter(audience!=\"Retargeting\"), \n    aes(x = mean, y = theme)) +\n  labs(title = \"Signups per $100: Comparing videos by audience\",\n    x = \"Density plots, means\") +\n  facet_wrap(~factor(audience, levels = audience_levels), scales = \"free_x\"  )\n)\n\n\n\n\n\nCode\n# \n# ggplot(filter(cost_tibble, upper_eti < 100)) +\n#   geom_errorbarh(aes(xmin = lower_eti, xmax = upper_eti, y = audience)) +\n#   geom_point(aes(x = mean, y = audience)) +\n#   labs(title = \"Cost per signup ($)\",\n#     x = \"Estimated cost per signup (USD) with 95% ETI\") +\n#   facet_wrap(~theme, scales = \"free_x\")\n\n\nAbove we consider the cost effectiveness of each video by audience.\n‘Factual short’: 23 This video seems to perform as good or better than any other video for each of the cause audiences as well as for the Lookalike audience (but see cav eat). It performs nearly as good as other videos for the other audiences.\nClimate video: This seems to have performed best for the Philanthropy and General audiences, although the distribution is diffuse. Surprisingly, it does not perform best for the climate audience.\nBrand Video, Factual Long: These seemed to have performed worst or near-worst for most audiences. ‘Factual long’ seems to have clearly performed worse than ‘Factual short’ version. (But see caveat below.)\n\n\n\n\n\n\nCaveat/todo – Video/campaign date imbalance\n\n\n\n\n\nThere is substantial imbalance in dates the administration of treatments in the pooled data. E.g., the ‘Factual short’ video was the only one shown for the 2021-12-08 trial, which may confound the above result. The Brand Video was only shown in a single trial, where it was the only video shown. The ‘Factual long’ video was dropped in later trials (possibly after poor performance in an explicit A/B test). Again, confounds are certainly possible. We may want to either remove trials from dates that only ran a single trial (at a first pass), or include start date/campaign into our modeling.\n\n\n\n\n\nCode\n(\n  sign_per_100d_plot_aud_by_vid <- cost_tibble %>%\n      filter(audience!=\"Retargeting\" & audience!=\"General audience\")  %>%\nggplot() +\n  scale_x_continuous(limits = c(0, 20)) +\n  ggridges::geom_density_ridges(\n    aes(\n      x = sign_per_100d, \n      y = audience\n      )\n    ) +\n  geom_point(data = cost_summary %>% filter(audience!=\"Retargeting\" & audience!=\"General audience\"), \n    aes(x = mean, y = audience)) +\n  labs(title = \"Signups per $100: Comparing audiences for each video\",\n    x = \"Density plots, means\") +\n  facet_wrap(~theme, scales = \"free_x\")\n)\n\n\n\n\n\nFlipping the previous plot, we compare the cost-effectiveness of each audience for each video.24, 25\nThe Lookalike audiences are relatively cost-effective for most videos, although the Philanthropy audience seems to do better per dollar for the Climate video.\nThe cause audiences are relatively cost-effective for their ‘own’ videos, but not overwhelmingly so. They seemed particularly cost-ineffective for the Brand video."
  },
  {
    "objectID": "chapters/gwwc_fb.html",
    "href": "chapters/gwwc_fb.html",
    "title": "2  Giving What We Can: Feb 22 Facebook Message Test",
    "section": "",
    "text": "Details in Gitbook HERE and Gdoc here\n\n\nCode\nknitr::include_url(\"https://effective-giving-marketing.gitbook.io/untitled/partner-organizations-and-trials/gwwc/feb-22-message-test\")"
  },
  {
    "objectID": "chapters/gwwc_fb.html#capturing-data",
    "href": "chapters/gwwc_fb.html#capturing-data",
    "title": "2  Giving What We Can: Feb 22 Facebook Message Test",
    "section": "2.2 Capturing data",
    "text": "2.2 Capturing data\n\n\n\n\n\n\nBringing in the data Erin coded\n\n\n\n\n\nAs a start, I source build work (Erin’s work, which I edited a bit) to bring in (and store) the data. I would do the coding a bit differently (more ‘tidyverse’ and less repetition), but it may not be worth redoing at this point.\n\n\n\n\n\nCode\n#this seems to be what Erin used ... but what is \n\nsource(here(\"gwwc\", \"build_GWWC_Feb_22_Message_test.R\"))"
  },
  {
    "objectID": "chapters/gwwc_pledge_wip.html",
    "href": "chapters/gwwc_pledge_wip.html",
    "title": "3  (GWWC: Pledge page trial)",
    "section": "",
    "text": "This trial is reported in the gitbook here. This mainly uses Google Analytics built in tools.\nThe present chapter is given as a placeholder, and for putting any supplementary analysis."
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html",
    "href": "chapters/oftw_upsell_input_first_analysis.html",
    "title": "4  OftW pre-GT-email upsell (impact/emotion)",
    "section": "",
    "text": "OftW pre-giving-tuesday-email upselling split test (considering ‘impact vs emotion’) c\n1"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#the-trial",
    "href": "chapters/oftw_upsell_input_first_analysis.html#the-trial",
    "title": "4  OftW pre-GT-email upsell (impact/emotion)",
    "section": "4.1 The trial",
    "text": "4.1 The trial\nIn December 2021, One For the World (OftW) sent out a sequence of emails to existing OftW pledgers/participants asking them for an additional donation. There were two ‘treatment variants’; an emotional email and a standard impact-based email. The treatment was constant by individual (the same person always got emails with the same theme).\nThe details are presented in our gitbook HERE and in the preregistration linked within (also on OSF)."
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#capturing-data",
    "href": "chapters/oftw_upsell_input_first_analysis.html#capturing-data",
    "title": "4  OftW pre-GT-email upsell (impact/emotion)",
    "section": "4.2 Capturing data",
    "text": "4.2 Capturing data\nKennan and Chloe captured the data and metadata from\n\nThe OFTW database\nSurveyMonkey\n\nPutting this into the (private) Google sheet linked HERE. We added some metadata/explainers to that data.2"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#input-and-clean-data",
    "href": "chapters/oftw_upsell_input_first_analysis.html#input-and-clean-data",
    "title": "4  OftW pre-GT-email upsell (impact/emotion)",
    "section": "4.3 Input and clean data",
    "text": "4.3 Input and clean data\nWe input the sheets from the external Google sheet location (access required)…\n\n\noftw_21_22_mc\ngs4_auth(scope = \"https://www.googleapis.com/auth/drive\")\ndrive_auth(token = gs4_token())\n\n#Mailchimp data\n\noftw_21_22_mc <- read_sheet(\"https://docs.google.com/spreadsheets/d/1iUKXkEqoadBgtUG_epBzUzCgA_J5mdWWstcXtpAdNJs/edit#gid=521638649\",\n  sheet=\"Raw data (Mailchimp)\")  %>%\n  dplyr::select(-`Treatment group`) %>%  #remove an un-useful repeated name column\n    mutate(`Treatment Group` = purrr::map_chr(`Treatment Group`, ~ .[[1]])) %>%\n  dplyr::select(`Email Address`, `Email Date`, `Treatment Group`, Opens, `Donate link clicks`, everything()) %>% #Most relevant features organized first\n  arrange(`Email Address`)\n\n\n#later: remove features brought in from OFTW database, reconstruct it\n\noftw_21_22_mc %>% names() %>% paste(collapse=\", \")\n\n\n[1] \"Email Address, Email Date, Treatment Group, Opens, Donate link clicks, sheet_descriptor, First Name, Last Name, Email Number, Class Year, Donor status, Total Given, Donation amount, Donation frequency, Start string, Platform, Portfolio string, Class lead, Impact 1, Impact 2, Impact 3, Employer, Pledge string, School string, Pledge year, Cancellation Type, Donation Amount String, OFTW matching, OFTW match amount, Corporate match amount, Bonuses announced, Post-bonus contact date, Email Preferences, Start Date, Lives Saved, Member Rating, Record rank, Total Giving Season contributions, Total Giving Season contribution amount\"\n\n\noftw_21_22_mc\n#...input descriptors for the above (do this later from \"doc: ...Mailchimp\" sheet\n\n\n\n\noftw_21_22_db_don\n#Donations data (and OftW database info)\n\noftw_21_22_db_don <- read_sheet(\n  \"https://docs.google.com/spreadsheets/d/1iUKXkEqoadBgtUG_epBzUzCgA_J5mdWWstcXtpAdNJs/edit#gid=521638649\",\n  sheet=\"Giving Season contributions (BigQuery)\") %>%\n  mutate(`Treatment group` = purrr::map_chr(`Treatment group`, ~ .[[1]])) %>%\n    dplyr::select(`email`, `primary_email`, `donation_date`, `Net_Contribution_Amount_USD`, `payment_platform`, everything()) %>%  #Most relevant features organized first\n  filter(donation_date>=as.Date(\"2021-11-15\")) # At least for now, remove pre-treatment donation data\n\noftw_21_22_db_don %>% names() %>% paste(collapse=\", \")\n\n\n[1] \"email, primary_email, donation_date, Net_Contribution_Amount_USD, payment_platform, Treatment group, Number of email opens, oftw_partner, school, chapter_type, portfolio, contribution_frequency, pledge_date, pledge_start_date, pledge_end_date, donor_status, cancellation_type, pledge_amount, pledge_contribution_frequency, x\"\n\n\n\n\n4.3.1 Labeling and cleaning\nThe code …\n…makes names snake_case, using original names as labels…\n\n\nCode\nlabelled::var_label(oftw_21_22_mc) <- names(oftw_21_22_mc)\nnames(oftw_21_22_mc) <- snakecase::to_snake_case(names(oftw_21_22_mc))\n\nlabelled::var_label(oftw_21_22_db_don) <- names(oftw_21_22_db_don)\nnames(oftw_21_22_db_don) <- snakecase::to_snake_case(names(oftw_21_22_db_don))\n\n\n\n…and anonymizes the data, hashing anything with the chance of being identifying\n\n\nCode\nsalty_hash <- function(x) {\n  salt(.seed = 42, x) %>% hash(.algo = \"crc32\")\n}\n\noftw_21_22_mc <- oftw_21_22_mc %>%\n  dplyr::select(-first_name, -last_name) %>%\n    mutate(across(c(email_address,  employer, school_string), salty_hash))\n\n\noftw_21_22_db_don <- oftw_21_22_db_don %>%\n      mutate(across(c(primary_email, email, school), salty_hash))\n\n\nRoll up to 1 per person; summarize and pivot_wider\n\n\nrollup\noutcomes_base_mc <- c(\"opens\", \"donate_link_clicks\")\n\n\noftw_21_22_mc_wide <- oftw_21_22_mc %>%\n  mutate(treat_emotion = case_when(\n    treatment_group == \"1.000000\" ~ 0,\n    treatment_group == \"2.000000\" ~ 1\n  )) %>%\n  group_by(email_address) %>%\n  mutate(across(all_of(outcomes_base_mc), sum, .names = \"{.col}_tot\")) %>%\n  tidyr::pivot_wider(names_from = email_number,\n    values_from = c(\"opens\", \"donate_link_clicks\")) %>%\n  mutate(d_click_don_link = donate_link_clicks_tot > 0) %>%\n  arrange(email_address) %>%\n  filter(row_number() == 1)\n\noftw_21_22_db_don <- oftw_21_22_db_don %>%\n  ungroup() %>%\n  group_by(email) %>%\n  mutate(\n    don_tot = sum(net_contribution_amount_usd),\n    num_don = n(),\n    d_don = num_don > 0,\n    don_tot_ot = sum(net_contribution_amount_usd[contribution_frequency ==\n        \"One-time\"]),\n    num_don_ot = sum(contribution_frequency == \"One-time\"),\n    d_don_ot = num_don_ot > 0,\n    #WAIT THIS IS NOT WIDE DATA -- don't interpret it as 'number of individuals'\n    don_tot_ss = sum(net_contribution_amount_usd[payment_platform == \"Squarespace\"]),\n    num_don_ss = sum(payment_platform == \"Squarespace\"),\n    d_don_ss = num_don_ss > 0,\n  )\n\n\noftw_21_22_db_don_persons <- oftw_21_22_db_don %>%\n  arrange(email) %>%\n  filter(row_number()==1)     %>%\nmutate(\n      treat_emotion = case_when(\n        treatment_group==\"1.000000\" ~ 0,\n        treatment_group == \"2.000000\" ~ 1)\n    )\n\n\noftw_mc_db <- power_full_join(oftw_21_22_mc_wide,\n   oftw_21_22_db_don_persons,  by = c(\"email_address\" = \"email\"), conflict = coalesce_xy) %>%\n   mutate(across(starts_with(\"d_don\"), ~replace(., is.na(.), 0)), #make it a 0 if it's not in the donation database\n   d_open= if_else(!is.na(treat_emotion),1,0)\n  )\n\n\noftw_mc_db_openers <- oftw_mc_db %>%\n  filter(!is.na(treat_emotion))\n\n# oftw_21_22_db_don_wide <- oftw_21_22_db_don %>%\n#   select(email, donation_date, net_contribution_amount_usd, payment_platform) %>%\n#    group_by(email) %>%\n#    mutate(row = row_number()) %>%\n#       tidyr::pivot_wider(names_from = row, values_from = c(\"donation_date\", \"net_contribution_amount_usd\"))\n\n\nPrelim results\n\n\nCode\noftw_mc_db %>% tabyl(treat_emotion, d_don)\n\n\n treat_emotion    0   1\n             0 1139 273\n             1  968 231\n            NA    0 395\n\n\nCode\noftw_mc_db %>% tabyl(treat_emotion, d_don_ss)\n\n\n treat_emotion    0 1\n             0 1404 8\n             1 1190 9\n            NA  391 4\n\n\nCode\noftw_21_22_db_don_persons %>% tabyl(treat_emotion, d_don_ot)\n\n\n treat_emotion FALSE TRUE NA_\n             0   248   15  10\n             1   215   12   4\n            NA   328   59   8\n\n\nCode\n#todo: simple statistical measures along with this\n\n\n\n\n4.3.2 Constructing outcome measures, especially ‘donations likely driven by email’\n\nDonations (presence, count, amounts) in giving seasons, 1 row per user (with treatment etc.)\n\n\noverall\nnon-regular\n‘likely from email’\n\nare in Giving Season contributions (BigQuery)\n\nsubset for payment platform = squarespace (unlikely to come from any other checkout page)\nemail as primary key, link to Raw Data (mailchimp), filter on ‘Donate link clicks`>0 (note that one needs aggregating by donor because it is ’per email’)\n\n\nGiving season donations ..\n\nGiving Season contributions (BigQuery), sum donation_date Net_Contribution_Amount_USD with filters for one-time etc\nCan check against fields ‘Total Giving Season contributions Total Giving Season contribution amount’\n\n\n\nCode\n#list/matrix of rates for later use\n\noc_mat <- oftw_mc_db %>%\n  mutate(d_open=n()) %>%\n  group_by(treat_emotion) %>%\n  dplyr::summarise(across(starts_with(\"d_\"), ~sum(.x, na.rm = TRUE), .names = \"tot_{.col}\"))\n\noc_mat_r <- oc_mat %>% filter(!is.na(treat_emotion)) #where treatment observed\n\nassigned_emails <- c(2000, 2000) #I was told that about 4000 emails were sent, 2000 to each group"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#descriptives-and-exploratory-analysis",
    "href": "chapters/oftw_upsell_input_first_analysis.html#descriptives-and-exploratory-analysis",
    "title": "4  OftW pre-GT-email upsell (impact/emotion)",
    "section": "4.4 Descriptives and exploratory analysis",
    "text": "4.4 Descriptives and exploratory analysis\nNotes:3\n\n4.4.1 Donation and outcome summary statistics, by treatment"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#basic-tests-donation-incidence-and-amounts",
    "href": "chapters/oftw_upsell_input_first_analysis.html#basic-tests-donation-incidence-and-amounts",
    "title": "4  OftW pre-GT-email upsell (impact/emotion)",
    "section": "4.5 Basic tests: Donation incidence and amounts",
    "text": "4.5 Basic tests: Donation incidence and amounts\n(See preregistration – go through preregistered tests one at a time. Note that given the observed conversion rates I do not expect any ‘significant differences’.)"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#basic-tests-clicks-and-retention-outcomes",
    "href": "chapters/oftw_upsell_input_first_analysis.html#basic-tests-clicks-and-retention-outcomes",
    "title": "4  OftW pre-GT-email upsell (impact/emotion)",
    "section": "4.6 Basic tests: Clicks and retention outcomes",
    "text": "4.6 Basic tests: Clicks and retention outcomes\nI’m following the approach discussed in the ‘RP methods discussion’ under “Significance and equivalence testing” with randomization inference/simulation; building to Bayes\nWe see a ‘small difference’ between treatment groups and it is ‘not significant in standard tests’ (tests not shown here yet). But can we put meaningful bounds on this? Can we statistically ‘rule out large effects’?\n(This parallels the analysis done in HERE, which includes some further explanation of the methods)\n\n\n\n4.6.1 Difference between two binomial random variables: Bayesian binomial test\n\n\nfill-in-data\n#would need to generate 'fill in data' if we want to use bayesAB, which requires actual vectors\n\n#add blank rows for 'assigned'\n\nblank_impact <- as_tibble(matrix(NA, nrow = assigned_emails[1]- oc_mat_r$tot_d_open[1], ncol = NCOL(oftw_mc_db)))\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\nUsing compatibility `.name_repair`.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nfill-in-data\nnames(blank_impact) <- names(oftw_mc_db)\n\nblank_impact %<>%\n  mutate(across(starts_with(\"d_\"), ~ifelse(is.na(.), 0, 0)),\n    treat_emotion=0)\n\nblank_emotion <- as_tibble(matrix(NA, nrow = assigned_emails[1]- oc_mat_r$tot_d_open[2], ncol = NCOL(oftw_mc_db)))\nnames(blank_emotion) <- names(oftw_mc_db)\n\nblank_emotion %<>%\n  mutate(across(starts_with(\"d_\"), ~ifelse(is.na(.), 0, 0)),\n    treat_emotion=1)\n\noftw_mc_db_assigned <-\n    bind_rows(oftw_mc_db, blank_impact, blank_emotion) %>%\n  filter(!is.na(treat_emotion))\n\noftw_mc_db_assigned %>% tabyl(treat_emotion)\n\n\n treat_emotion    n percent\n             0 2000     0.5\n             1 2000     0.5\n\n\nOpens by treatment:\n\n\nCode\n# Following r https://www.sumsar.net/blog/2014/06/bayesian-first-aid-prop-test/\n# alt: http://frankportman.github.io/bayesAB/\n\n\n#opens_by_treat_fit <- bayes.prop.test(oc_mat_r$tot_d_open, assigned_emails, cred.mass = 0.95) #here I highlight the 95% bounds because it's a strong effect!\n\n#plot(opens_by_treat_fit)\n\nunif_prior <- c('alpha' = 1, 'beta' = 1)\n\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_open), 'beta' = sum(assigned_emails))\n\n\n#same with AB  package\n# Fit bernoulli test\nopens_by_treat_AB <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_open[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_open[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\nopens_by_treat_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_open[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_open[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\nplot(opens_by_treat_AB)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(opens_by_treat_AB_empir)\n\n\n\n\n\n\n\n\n\n\n\nCode\n#WTF -- I need to advance this by command prompt!?\n\n#AB1 <- bayesTest(oftw_21_22_db_don_persons$d_don_ot[trea], B_binom, priors = c('alpha' = 65, 'beta' = 200), n_samples = 1e5, distribution = 'bernoulli')\n\n\n\n\n‘Some donation’ by treatment (only for those who opened, otherwise donations are surely undercounted for Emotion treatment)\n\n\nCode\noftw_21_22_db_don_persons %>% tabyl(treat_emotion, d_don)\n\n\n treat_emotion TRUE\n             0  273\n             1  231\n            NA  395\n\n\nCode\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_don), 'beta' = sum(oc_mat_r$tot_d_open))\n\n(\n  don_by_treat_opened_fit <- bayes.prop.test(oc_mat_r$tot_d_don, oc_mat_r$tot_d_open, cred.mass = 0.80) # 80% credible interval for decision-making purposes\n)\n\n\n\n    Bayesian First Aid proportion test\n\ndata: oc_mat_r$tot_d_don out of oc_mat_r$tot_d_open\nnumber of successes:   273,  231\nnumber of trials:     1412, 1199\nEstimated relative frequency of success [80% credible interval]:\n  Group 1: 0.19 [0.18, 0.21]\n  Group 2: 0.19 [0.18, 0.21]\nEstimated group difference (Group 1 - Group 2):\n  0 [-0.02, 0.02]\nThe relative frequency of success is larger for Group 1 by a probability\nof 0.515 and larger for Group 2 by a probability of 0.485 .\n\n\nCode\nplot(don_by_treat_opened_fit)\n\n\n\n\n\nCode\n#same with AB  package\n# Fit bernoulli test\ndon_by_treat_AB <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don[oftw_mc_db_openers$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\ndon_by_treat_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don[oftw_mc_db_openers$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\nplot(don_by_treat_AB)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(don_by_treat_AB_empir)\n\n\n\n\n\n\n\n\n\n\n\nThus we put 80% probability on the difference between the donation rates being no more than (hard-coded) .023/.19 = 12% in either direction. Note that this is not terribly narrowly bounded.\n\n\nNext, for one-time donations only; again as a share of opens\n\n\nCode\n(\n  don_ot_by_treat_opened_fit <- bayes.prop.test(oc_mat_r$tot_d_don_ot, oc_mat_r$tot_d_open, cred.mass = 0.80) # 80% credible interval for decision-making purposes\n)\n\n\n\n    Bayesian First Aid proportion test\n\ndata: oc_mat_r$tot_d_don_ot out of oc_mat_r$tot_d_open\nnumber of successes:    15,   12\nnumber of trials:     1412, 1199\nEstimated relative frequency of success [80% credible interval]:\n  Group 1: 0.011 [0.0075, 0.015]\n  Group 2: 0.011 [0.0068, 0.014]\nEstimated group difference (Group 1 - Group 2):\n  0 [-0.0048, 0.0056]\nThe relative frequency of success is larger for Group 1 by a probability\nof 0.55 and larger for Group 2 by a probability of 0.45 .\n\n\nCode\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_don_ot), 'beta' = sum(oc_mat_r$tot_d_open))\n\n#same with AB  package\n# Fit bernoulli test\ndon_ot_by_treat_AB <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don_ot[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don_ot[oftw_mc_db_openers$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\ndon_ot_by_treat_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don_ot[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don_ot[oftw_mc_db_openers$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\nplot(don_ot_by_treat_AB)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(don_ot_by_treat_AB_empir)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndon_ot_by_treat_AB %>% summary(credInt=0.9)\n\n\nQuantiles of posteriors for A and B:\n\n$Probability\n$Probability$A\n         0%         25%         50%         75%        100% \n0.002702340 0.009300256 0.011076158 0.013056869 0.028481793 \n\n$Probability$B\n         0%         25%         50%         75%        100% \n0.002578133 0.008699817 0.010557270 0.012664164 0.031399003 \n\n\n--------------------------------------------\n\nP(A > B) by (0)%: \n\n$Probability\n[1] 0.5518\n\n--------------------------------------------\n\nCredible Interval on (A - B) / B for interval length(s) (0.9) : \n\n$Probability\n       5%       95% \n-0.434144  0.969118 \n\n--------------------------------------------\n\nPosterior Expected Loss for choosing A over B:\n\n$Probability\n[1] 0.1618676\n\n\nCode\ndon_ot_by_treat_AB_lift_int80 <- don_ot_by_treat_AB %>% summary(credInt=0.8)\n\n\ndon_ot_by_treat_AB_lift_int80_empir <- don_ot_by_treat_AB_empir %>% summary(credInt=0.8)\n\n\n80% credible intervals with the uniform prior for the ‘lift’ of Impact relative to Emotion are\n-0.351, 0.707\nand for the empirically informed (but symmetric prior):\n-0.234, 0.357\n(Hard-coded) Here there is just a trace of suggestive evidence that the emotion treatment performed worse. But even our 80% bounds are very wide.\n\n\nFor ‘Squarespace donations only’; these are the donations that plausibly came from the email. First as a share of opens for each treatment :\n\n\nCode\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_don_ss), 'beta' = sum(oc_mat_r$tot_d_open))\n\n\n(\n  don_ss_by_treat_opened_fit <- bayes.prop.test(oc_mat_r$tot_d_don_ss, oc_mat_r$tot_d_open, cred.mass = 0.80) # 80% credible interval for decision-making purposes\n)\n\n\n\n    Bayesian First Aid proportion test\n\ndata: oc_mat_r$tot_d_don_ss out of oc_mat_r$tot_d_open\nnumber of successes:     8,    9\nnumber of trials:     1412, 1199\nEstimated relative frequency of success [80% credible interval]:\n  Group 1: 0.0062 [0.0033, 0.0085]\n  Group 2: 0.0081 [0.0047, 0.011]\nEstimated group difference (Group 1 - Group 2):\n  0 [-0.0062, 0.0023]\nThe relative frequency of success is larger for Group 1 by a probability\nof 0.282 and larger for Group 2 by a probability of 0.718 .\n\n\nCode\n#same with AB  package\n# Fit bernoulli test\ndon_ss_by_treat_AB <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don_ss[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don_ss[oftw_mc_db_openers$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\ndon_ss_by_treat_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don_ss[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don_ss[oftw_mc_db_openers$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\n\nplot(don_ss_by_treat_AB)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(don_ss_by_treat_AB_empir)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndon_ss_by_treat_AB %>% summary(credInt=0.9)\n\n\nQuantiles of posteriors for A and B:\n\n$Probability\n$Probability$A\n          0%          25%          50%          75%         100% \n0.0009902386 0.0048437368 0.0061367516 0.0076340142 0.0191421384 \n\n$Probability$B\n         0%         25%         50%         75%        100% \n0.001462728 0.006437438 0.008042132 0.009891706 0.025724099 \n\n\n--------------------------------------------\n\nP(A > B) by (0)%: \n\n$Probability\n[1] 0.27906\n\n--------------------------------------------\n\nCredible Interval on (A - B) / B for interval length(s) (0.9) : \n\n$Probability\n        5%        95% \n-0.6515304  0.6391667 \n\n--------------------------------------------\n\nPosterior Expected Loss for choosing A over B:\n\n$Probability\n[1] 0.5346835\n\n\nCode\ndon_ss_by_treat_AB_lift_int80 <- don_ss_by_treat_AB %>% summary(credInt=0.8)\n\n\ndon_ss_by_treat_AB_lift_int80_empir <- don_ss_by_treat_AB_empir %>% summary(credInt=0.8)\n\n\n80% credible intervals with the uniform prior for the ‘lift’ of Impact relative to Emotion are\n-0.583, 0.383\nand for the empirically informed (but symmetric prior):\n-0.367, 0.304\n(Hard-coded) Again, even our 80% bounds are very wide.\n\n\nFinally, we consider the above as a share of total emails sent, allowing that ‘opens’ is non-random,\n… and also implicitly assuming that the only impact of these treatments could be on the Squarespace donations made by someone who did open the email.\n\n\nCode\n(\n  don_ss_by_treat_opened_fit_all <- bayes.prop.test(oc_mat_r$tot_d_don_ss, assigned_emails,\n    cred.mass = 0.80) # 80% credible interval for decision-making purposes\n)\n\n\n\n    Bayesian First Aid proportion test\n\ndata: oc_mat_r$tot_d_don_ss out of assigned_emails\nnumber of successes:     8,    9\nnumber of trials:     2000, 2000\nEstimated relative frequency of success [80% credible interval]:\n  Group 1: 0.0043 [0.0025, 0.0062]\n  Group 2: 0.0049 [0.0029, 0.0067]\nEstimated group difference (Group 1 - Group 2):\n  0 [-0.0033, 0.0022]\nThe relative frequency of success is larger for Group 1 by a probability\nof 0.405 and larger for Group 2 by a probability of 0.595 .\n\n\nCode\n#\n#same with AB package\n\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_don_ss), 'beta' = sum(assigned_emails))\n\n\n# Fit Bernoulli test\ndon_ss_by_treat_all_AB <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_don_ss[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_don_ss[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\ndon_ss_by_treat_all_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_don_ss[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_don_ss[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\nplot(don_ss_by_treat_all_AB)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(don_ss_by_treat_all_AB_empir)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndon_ss_by_treat_all_AB %>% summary(credInt=0.9)\n\n\nQuantiles of posteriors for A and B:\n\n$Probability\n$Probability$A\n          0%          25%          50%          75%         100% \n0.0006591401 0.0034229384 0.0043367467 0.0053948239 0.0139389081 \n\n$Probability$B\n          0%          25%          50%          75%         100% \n0.0008503472 0.0038633646 0.0048302873 0.0059443205 0.0137935894 \n\n\n--------------------------------------------\n\nP(A > B) by (0)%: \n\n$Probability\n[1] 0.4076\n\n--------------------------------------------\n\nCredible Interval on (A - B) / B for interval length(s) (0.9) : \n\n$Probability\n        5%        95% \n-0.5874267  0.9235146 \n\n--------------------------------------------\n\nPosterior Expected Loss for choosing A over B:\n\n$Probability\n[1] 0.353655\n\n\nCode\ndon_ss_by_treat_all_AB_lift_int80 <- don_ss_by_treat_all_AB %>% summary(credInt=0.8)\n\n\ndon_ss_by_treat_all_AB_lift_int80_empir <- don_ss_by_treat_all_AB_empir %>% summary(credInt=0.8)\n\n\n80% credible intervals with the uniform prior for the ‘lift’ of Impact relative to Emotion are\n\n\nCode\n op(don_ss_by_treat_all_AB_lift_int80$interval$Probability)\n\n\n     10%      90% \n\"-0.507\" \" 0.621\" \n\n\nand for the empirically informed (but symmetric prior):\n\n\nCode\nop(don_ss_by_treat_all_AB_lift_int80_empir$interval$Probability)\n\n\n     10%      90% \n\"-0.329\" \" 0.379\" \n\n\nHard-coded: Here there is almost no evidence in either direction. However, our 80% credible intervals remain wide.\n\n\nUnfortunately, this experiment proved to be underpowered, at least for the donation outcome.\nBut what about clicks on the ‘donation link’? This could arguably be seen as a measure of ‘desire and intention to donate’, and thus might be a more fine-grained and less noisy measure, improving our statistical power.\nSome quick crosstabs (here as a share of opens)\n\n\nCode\n(\n  donclick_by_treat <-  oftw_mc_db %>%\n  filter(!is.na(treat_emotion)) %>%\n  tabyl(treat_emotion, d_click_don_link) %>%\n    adorn_percentages(\"row\") %>%\n    adorn_pct_formatting() %>%\n    adorn_ns() %>%\n    adorn_title() %>%\n    kable(caption =\"Click on donation link by treatment; all opened emails\")  %>%\n  kable_styling(latex_options = \"scale_down\")\n)\n\n\n\n\nClick on donation link by treatment; all opened emails\n \n  \n     \n    d_click_don_link \n     \n     \n  \n \n\n  \n    treat_emotion \n    FALSE \n    TRUE \n    NA_ \n  \n  \n    0 \n    97.5% (1376) \n    2.1% (29) \n    0.5% (7) \n  \n  \n    1 \n    94.6% (1134) \n    4.7% (56) \n    0.8% (9) \n  \n\n\n\n\n\nIf this is our metric, it only seems fair to take into account ‘whether they opened the email’ as part of this effect. Thus, considering clicks as a share of total emails sent…\n\n\nCode\n(\n  click_don_by_treat_opened_fit <- bayes.prop.test(oc_mat_r$tot_d_click_don_link, assigned_emails,\n    cred.mass = 0.95) # 80% credible interval for decision-making purposes\n)\n\n\n\n    Bayesian First Aid proportion test\n\ndata: oc_mat_r$tot_d_click_don_link out of assigned_emails\nnumber of successes:    29,   56\nnumber of trials:     2000, 2000\nEstimated relative frequency of success [95% credible interval]:\n  Group 1: 0.015 [0.0098, 0.020]\n  Group 2: 0.028 [0.021, 0.036]\nEstimated group difference (Group 1 - Group 2):\n  -0.01 [-0.023, -0.0046]\nThe relative frequency of success is larger for Group 1 by a probability\nof 0.002 and larger for Group 2 by a probability of 0.998 .\n\n\nCode\nplot(click_don_by_treat_opened_fit)\n\n\n\n\n\n\n\ndon_click_by_treat_all_AB\n#same with AB  package\n\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_click_don_link), 'beta' = sum(assigned_emails))\n\noftw_mc_db_assigned <- oftw_mc_db_assigned %>% dplyr::filter(!is.na(d_click_don_link))\n\n# Fit bernoulli test\ndon_click_by_treat_all_AB <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_click_don_link[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_click_don_link[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\ndon_click_by_treat_all_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_click_don_link[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_click_don_link[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\nplot(don_click_by_treat_all_AB)\n\n\n\n\n\n\n\n\n\n\n\ndon_click_by_treat_all_AB\nplot(don_click_by_treat_all_AB_empir)\n\n\n\n\n\n\n\n\n\n\n\ndon_click_by_treat_all_AB\ndon_click_by_treat_all_AB %>% summary(credInt=0.9)\n\n\nQuantiles of posteriors for A and B:\n\n$Probability\n$Probability$A\n         0%         25%         50%         75%        100% \n0.005613925 0.013111564 0.014855512 0.016785020 0.030921759 \n\n$Probability$B\n        0%        25%        50%        75%       100% \n0.01457222 0.02602580 0.02846221 0.03102267 0.04764887 \n\n\n--------------------------------------------\n\nP(A > B) by (0)%: \n\n$Probability\n[1] 0.00155\n\n--------------------------------------------\n\nCredible Interval on (A - B) / B for interval length(s) (0.9) : \n\n$Probability\n        5%        95% \n-0.6412379 -0.2472723 \n\n--------------------------------------------\n\nPosterior Expected Loss for choosing A over B:\n\n$Probability\n[1] 0.9686057\n\n\ndon_click_by_treat_all_AB\ndon_click_by_treat_all_AB_lift_int80 <- don_click_by_treat_all_AB %>% summary(credInt=0.8)\n\ndon_click_by_treat_all_AB_lift_int80_empir <- don_click_by_treat_all_AB_empir %>% summary(credInt=0.8)\n\n\n80% credible intervals with the uniform prior for the ‘lift’ of Impact relative to Emotion are\n-0.610, -0.304\nand for the empirically informed (but symmetric prior):\n-0.3119, -0.0531\n(Hard-coded)\nThere is fairly strong evidence that the emotion email lead to a higher rate of clicks on the donation link; note that this even is in spite of the lower rate of email opens.\nThis suggests (IMO) it is worth testing this further.\n\n\n4.6.2 Redoing a bunch of stuff manually\nFirst, some building blocks;\nthe probability distribution over the absolute value of differences between two binomial random variables\nAdapting formulas from Stackexchange post discussion\nDefining their code for this function:\n\n\nCode\nmodBin <- dbinom #DR: I just do this renaming here for consistency with the rest ... but the modBin they defined was redundant as it's already built ins\n\ndiffBin <- function(z, n1, p1, n2, p2){\n\n  prob <- 0\n\n  if (z>=0){\n    for (i in 1:n1){\n      prob <- prob + modBin(i+z, n1, p1) * modBin(i, n2, p2)\n    }\n\n  }\n  else\n  {\n    for (i in 1:n2){\n      prob<-prob+modBin(i+z, n1, p1)*modBin(i, n2, p2)\n    }\n  }\n  return(prob)\n}\n\n\n\n\nWe generate an alternate version to cover ‘differences in one direction, i.e., but ’probability of observing (d1-d2)/(n1+n2) share more of d1 responses than d2 responses given sample sizes n1 and n2… over a range of true probabilities p1 and p2’\nthe probability distribution for differences between two binomial random variables in one direction\n\n\n\nFor the present case\nHard-coded notes…\n::: {.foldable}\n\n\n\nCode\nn1 <- oc_mat_r$tot_d_open[1]\nn2 <- oc_mat_r$tot_d_open[2]\nd1 <- oc_mat_r$tot_d_click_don_link[1]\nd2 <- oc_mat_r$tot_d_click_don_link[2]\nz <- d1-d2 #impact minus emotion\n\n\nComputation for a few ‘ad-hoc cases’ (later explore the space with vectors of values)\n\nSuppose truly equal incidence, at the mean level\n\n\n\nCode\np1 <- (d1+d2)/(n1+n2)\n\np2 <- p1\n\n(\n  db_0 <- diffBin(z, n1, p1, n2, p2)\n)\n\n\n[1] 3.963627e-05\n\n\nThis implies there is a 0.00396% chance of getting this exact difference of +-27 incidence(s) between the treatments (in one direction), if the true incidence rates were equal.\nLet’s plot this for a range of ‘incidence rate differences’ in this region. (Sorry, using the traditional plot, ggplot is better).\n\n\nCode\ns <- seq(-10*z, 10*z)\np<-sapply(s, function(z) diffBin(z, n1, p1, n2, p2))\nplot(s,p)\n\n\n\n\n\nWe see a large likelihood of values in the range of the +-27 difference observed, and a low likelihood of a difference of 10 or more in either direction.\n\n\n4.6.3 Adaptation: ‘of this magnitude or smaller’\n\n\nCode\nltmag_diffBin <- function(z, n1, p1, n2, p2){\n  prob <- 0\n  z_n <- -z #negative value\n\n  for (i in z_n:z){     #sum for all integer differences between observed value and its negative, inclusive\n    prob <- prob + diffBin(i, n1, p1, n2, p2)\n    }\n\n  return(prob)\n}\n\n\nNow, a similar computation as above, but for ‘a difference this big or smaller in magnitude’:\n\n\nCode\n  (\n    mag_db_0 <- ltmag_diffBin(z, n1, p1, n2, p2)\n  )\n\n\n[1] 0.9880585\n\n\nThis implies there is a 98.8% chance of getting a difference no larger than this one in magnitude of +/–27 incidences between the treatments if the true incidence rates were equal.\n\n\nAnd finally, what we were looking for: the chance of ‘a difference this small or smaller’ as a function of the true difference…\n(Think about: should we do this for ‘a difference in the same direction’ instead?)\nSet up an arbitrary vector of ‘true differences’ \nBelow, I plot\nY-axis: ’how likely would be a difference in donations ‘as small or smaller in magnitude’” than we see in the data against…\nX-axis: if the “true difference in incidence rates” were of these magnitudes\n(Note: this should cover ‘a difference in either direction’; the probability of a difference in the direction we do see is obviously somewhat smaller)\n\n\nCode\noptions(scipen=999)\n\nB <- c(1, 1.5, 2, 2.5, 3)\n\np1 <- rep((d1+d2)/(n1+n2), length(B))\np2 <- p1*B\n\n\nas.list(ltmag_diffBin(z, n1, p1, n2, p2)*100) %>% format(digits=3, scientific=FALSE)\n\n\n[1] \"98.8\"   \"93.2\"   \"33.6\"   \"1.81\"   \"0.0157\"\n\n\nCode\nprobmag <- ltmag_diffBin(z, n1, p1, n2, p2)\n\n\n#qplot(B, probmag, log  = \"x\", xlab = \"True relative incidence\", ylab =\"Prob. of difference this small\")\n\n(\n  probmag_plot <-\n    ggplot() +\n  aes(x=B, y=probmag) +\n  geom_point() +\n  scale_x_continuous(trans='log2') +\n    ylim(0,1) +\n    xlab(\"True relative incidence rate\") +\n    ylab(\"Prob. diff. as small as obsd\")\n\n)\n\n\n\n\n\nHard-coded takeaways 15 Dec 2021 :\nOur data is consistent with ‘no difference’ (of course) … but its also consistent with ‘a fairly large difference in incidence’\nE.g., even if one treatment truly lead to ‘twice as many donations as the other’, we still have a 20% chance of seeing a differences as small as the one we see (of 8 versus 6)\nWe can reasonably ‘rule out’ differences of maybe 2.5x or greater\nMain point: given the rareness of donations in this context, our sample size doesn’t let us make very strong conclusions in either directions … at least not yet. I hope that combined with other evidence, we will be able to infer more\n\n\n4.6.4 Quick redo assuming equal shares recieved each email, and treating ‘email reciepts as denom’\nApprox 4000 total emails sent?\nFor squarespace\n\n\nCode\nn1 <- 2000\nn2 <- 2000\nd1 <- 10\nd2 <- 9\nz <- d1-d2\n\nB <- c(1/3, 1/2.5, 1/2, 1/1.5, 1, 1.5, 2, 2.5, 3)\n\np1 <- rep((d1+d2)/(n1+n2), length(B))\np2 <- p1*B\n\n\n\n(\n    mag_db_0_ss <- ltmag_diffBin(z, n1, p1, n2, p2)\n  )\n\n\n[1] 0.071197896 0.100275616 0.147727269 0.220724651 0.272100392 0.154218060\n[7] 0.046306831 0.009174658 0.001345637\n\n\nCode\nprobmag_ss <- ltmag_diffBin(z, n1, p1, n2, p2)\n\n\n(\n  probmag_plot_ss <-\n    ggplot() +\n  aes(x=B, y=probmag_ss) +\n  geom_point() +\n  scale_x_continuous(trans='log2') +\n    ylim(0,.51) +\n    xlab(\"True relative incidence rate\") +\n    ylab(\"Prob. diff. as small as obsd\")\n\n)\n\n\n\n\n\nCode\n#note that it is not symmetric bc (I think) a very low incidence on one side makes particular large observed proportional differences more likely\n\n\nFor all one-time donations\n\n\nCode\nn1 <- 2000\nn2 <- 2000\nd1 <- 15\nd2 <- 12\nz <- d1-d2\n\nB <- c(1/3, 1/2.5, 1/2, 1/1.5, 1, 1.5, 2, 2.5, 3)\n\np1 <- rep((d1+d2)/(n1+n2), length(B))\np2 <- p1*B\n\n\n(\n    mag_db_0 <- ltmag_diffBin(z, n1, p1, n2, p2)\n  )\n\n\n[1] 0.0922168538 0.1395427545 0.2249330548 0.3745240308 0.5032003013\n[6] 0.2505329859 0.0521354769 0.0059810731 0.0004413855\n\n\n(the below halts on build, so I commented it out)\n\n\nprobmag_plot_ot\n(\n  probmag_plot_ot <-\n    ggplot() +\n  aes(x=B, y=probmag) +\n  geom_point() +\n  scale_x_continuous(trans='log2') +\n    ylim(0,.51) +\n    xlab(\"True relative incidence rate\") +\n    ylab(\"Prob. diff. as small as obsd\")\n\n)"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#demographics-and-interactions-to-do",
    "href": "chapters/oftw_upsell_input_first_analysis.html#demographics-and-interactions-to-do",
    "title": "4  OftW pre-GT-email upsell (impact/emotion)",
    "section": "4.7 Demographics and interactions (to do)",
    "text": "4.7 Demographics and interactions (to do)"
  },
  {
    "objectID": "chapters/tlycs_input_simple_analysis.html",
    "href": "chapters/tlycs_input_simple_analysis.html",
    "title": "5  TLYCS Portland trial: Brief report",
    "section": "",
    "text": "Background, data input, brief report"
  },
  {
    "objectID": "chapters/tlycs_input_simple_analysis.html#the-trial",
    "href": "chapters/tlycs_input_simple_analysis.html#the-trial",
    "title": "5  TLYCS Portland trial: Brief report",
    "section": "5.1 The trial",
    "text": "5.1 The trial\nIn December 2021, TLYCS ran a YouTube advertising campaign in single city, involving ‘donation advice’. The top 10% household income households were targeted with (one of) three categories of videos. One of the ultimate goals was to get households to sign up for a ‘concierge’ personal donor advising service.\nThe details are presented in our gitbook HERE\n\nQuick takeaways\nThere were very few signups for the concierge advising service. (About 16 in December 2021 , only 1 from Portland.)\nWe consider a ‘difference in difference’, to compare the year-on-year changes in visits to TLYCS during this period for Portland vs other comparison cities.\nThis comparison yields a ‘middle estimate cost’ of $37.7 per additional visitor to the site. This seems relatively expensive. We could look into this further to build a more careful model and consider statistical bounds, if such work is warranted."
  },
  {
    "objectID": "chapters/tlycs_input_simple_analysis.html#capturing-data",
    "href": "chapters/tlycs_input_simple_analysis.html#capturing-data",
    "title": "5  TLYCS Portland trial: Brief report",
    "section": "5.2 Capturing data",
    "text": "5.2 Capturing data\n\n\n\n\n\n\nGoogle Analytics capture\n\n\n\n\n\nI (David Reinstein) did a manual ‘create report and download’ in TLYCS’s Google Analytics, basically as described here.\nThe Google Analytics report and it’s parameters can be accessed here, if you have access.\n\n\n\n\n\n\n\n\n\nGoogle Analytics report: parameters and data storage\n\n\n\n\n\nReport:\n\n3-31 December 2021 vs prior year, same dates\nAll North American cities with 1 or more user\nCounts: Users, sessions, certain types of conversions (see below)\n\nI downloaded this as an Excel spreadsheet to the private eamt_actual_data repo:\neamt_actual_data/tlycs/tlycs_dec_2021_vs_2020_by_city_n_america.xlsx"
  },
  {
    "objectID": "chapters/tlycs_input_simple_analysis.html#input-and-clean-data",
    "href": "chapters/tlycs_input_simple_analysis.html#input-and-clean-data",
    "title": "5  TLYCS Portland trial: Brief report",
    "section": "5.3 Input and clean data",
    "text": "5.3 Input and clean data\n1\n\n\nReading TLYCS trial data\n#tl21 <- read_excel(here::here(\"data_do_not_commit\", \"tlycs\", \"tlycs_dec_2021_vs_2020_by_city_n_america.xlsx\"), sheet = \"Dataset1\") #this should have worked?\n\n#tl21 <- readxl::read_excel(\"~/githubs/eamt_actual_data/tlycs/tlycs_dec_2021_vs_2020_by_city_n_america.xlsx\",  sheet = \"Dataset1\")\n\n\ntl21 <- readxl::read_excel(here::here(\"tlycs_portland_trial_2021\", \"tlycs_dec_2021_vs_2020_by_city_n_america.xlsx\"),  sheet = \"Dataset1\") %>%\n  dplyr::as_tibble()\n\n\nWe add companion data on US city sizes.2\n\n\ninput cities data\nus_cities_pop <- read.csv(here::here(\"tlycs_portland_trial_2021\", \"uscities_pop.csv\")) %>%\n  dplyr::select(name, rank, usps, pop2022) %>%\n  mutate(name = if_else(name==\"New York City\", \"New York\", name)) %>%\n  as_tibble()\n\ntl21 <- left_join(\n  tl21,\n  us_cities_pop,\n by = c(\"City\" = \"name\") ) %>%\n  mutate(\n    pop_gt250k = pop2022>250000 | str_det(City, \"Windsor|Oshawa|Halifax|Victoria|Kitchener|Hamilton|Quebec|Winnipeg|Ottawa|Edmonton|Calgary|Vancouver|Montreal|Toronto\")\n  ) %>%\n  dplyr::select(City, `Date Range`, Users, pop2022, pop_gt250k, everything())\n\n\n\n\n\n\n\n\nMore coding\n\n\n\n\n\nBelow, we:\n\nChoose the ‘largest Portland’ as this must be Portland, OR\nDrop other city duplicates to enable lagging/leading more easily\nCreates lags and ‘differences across years’ features.\n\n\n\n\n\n\nCreate ‘year’ feature\ntl21 <- tl21 %>% mutate(\n  year = case_when(\n    str_detect(`Date Range`, \"2021\") == TRUE ~ 2021,\n    str_detect(`Date Range`, \"2020\") == TRUE ~ 2020\n  )\n)\n\n\n\n\nTable of ‘Portland’ cities\n# Re-labels the 'correct' Portland (we think), as it's a common city name.^[And there are two other 'Portlands', each with a small number of sessions]\n\ntl21 %>%\n  filter(City==\"Portland\") %>%\n  dplyr::select(City, year, Users, pop2022, pop_gt250k) %>%\n  .kable() %>%\n  .kable_styling\n\n\n\n\n \n  \n    City \n    year \n    Users \n    pop2022 \n    pop_gt250k \n  \n \n\n  \n    Portland \n    2,021 \n    1 \n    666,453 \n    TRUE \n  \n  \n    Portland \n    2,020 \n    0 \n    666,453 \n    TRUE \n  \n  \n    Portland \n    2,021 \n    8 \n    666,453 \n    TRUE \n  \n  \n    Portland \n    2,020 \n    6 \n    666,453 \n    TRUE \n  \n  \n    Portland \n    2,021 \n    306 \n    666,453 \n    TRUE \n  \n  \n    Portland \n    2,020 \n    144 \n    666,453 \n    TRUE \n  \n\n\n\n\n\n\n\nrelable portland\n#Presumably the largest one is Portland, Oregon, and we'll re-label it as such.\n\ntl21 <- tl21 %>%\n  mutate(\n    City = case_when(City==\"Portland\" & Users>20 ~ \"Portland_OR\",\n              City==\"Portland\" ~ \"Other_Portland\",\n              TRUE ~ City)\n  )\n\n\n\n\ndrop_dup_cities\ntl21 <- tl21 %>%\n  rowwise() %>%\n   mutate(key = paste(sort(c(City, year)), collapse=\"\")) %>%\n   distinct(key, .keep_all=T) %>%\n   dplyr::select(-key) %>%\n  ungroup\n\n\n\n\nCreate lag and difference features\ntlag <- function(x, n = 1L, time) {\n  index <- match(time - n, time, incomparables = NA)\n  x[index]\n}\n\ntl21 <- tl21 %>%\n  group_by(City) %>%\n  mutate(\n  lag_users = tlag(Users, 1, time = year),\n  diff_users = Users-lag_users,\n    prop_diff_users = diff_users/lag_users\n    ) %>%\n  ungroup()"
  },
  {
    "objectID": "chapters/tlycs_input_simple_analysis.html#casualsimple-uptick-analysis",
    "href": "chapters/tlycs_input_simple_analysis.html#casualsimple-uptick-analysis",
    "title": "5  TLYCS Portland trial: Brief report",
    "section": "5.4 Casual/simple ‘uptick’ analysis",
    "text": "5.4 Casual/simple ‘uptick’ analysis\nBelow, for the comparable periods in 2020 and 2021, we give…\n…the total numbers of cities in the sample, the share with a positive number of user clicks, and the mean, median, 80th quantile, and standard deviation of the number of clicks.\n… For a few subsets\n… First for unique user visits, and then for total numbers of sessions.\n\n\ntabs of session by year\ntl21 %>% sumtab(Users, year, caption=\"All N. Amer. cities\")\n\n\n\n\nAll N. Amer. cities\n \n  \n    year \n    N \n    share > 0 \n    Mean \n    Median \n    P80 \n    Std.dev. \n  \n \n\n  \n    2020 \n    3006 \n    0.6 \n    9.38 \n    1 \n    4 \n    (296.34) \n  \n  \n    2021 \n    3006 \n    1.0 \n    13.48 \n    2 \n    4 \n    (425.33) \n  \n\n\n\n\n\ntabs of session by year\ntl21 %>% filter(!City==\"Portland_OR\") %>% sumtab(Users, year, caption=\"N. Amer. Cities other than Portland\")\n\n\n\n\nN. Amer. Cities other than Portland\n \n  \n    year \n    N \n    share > 0 \n    Mean \n    Median \n    P80 \n    Std.dev. \n  \n \n\n  \n    2020 \n    3004 \n    0.6 \n    3.94 \n    1 \n    4 \n    (21.01) \n  \n  \n    2021 \n    3004 \n    1.0 \n    5.64 \n    2 \n    4 \n    (30.79) \n  \n\n\n\n\n\ntabs of session by year\ntl21 %>% filter(pop2022>250000) %>% sumtab(Users, year, caption=\"All N. Amer. Cities with pop. > 250k\") #Fix -- filter on over 20 users for 2020 ONLY]\n\n\n\n\nAll N. Amer. Cities with pop. > 250k\n \n  \n    year \n    N \n    share > 0 \n    Mean \n    Median \n    P80 \n    Std.dev. \n  \n \n\n  \n    2020 \n    92 \n    0.848 \n    47.46 \n    17.0 \n    47.0 \n    (99.21) \n  \n  \n    2021 \n    92 \n    1.000 \n    70.04 \n    19.5 \n    80.2 \n    (147.88) \n  \n\n\n\n\n\ntabs of session by year\ntl21 %>% sumtab(Sessions, year, caption=\"Sessions by year, all\")\n\n\n\n\nSessions by year, all\n \n  \n    year \n    N \n    share > 0 \n    Mean \n    Median \n    P80 \n    Std.dev. \n  \n \n\n  \n    2020 \n    3006 \n    0.6 \n    11.75 \n    1 \n    4 \n    (370.61) \n  \n  \n    2021 \n    3006 \n    1.0 \n    16.80 \n    2 \n    5 \n    (528.67) \n  \n\n\n\n\n\nNote that all measures generally show an increase from year to year.\n\n\nUpticks for Oregon, other\nOR_users_21 <- tl21 %>% dplyr::filter(City==\"Portland_OR\" & year==2021) %>% dplyr::select('Users') %>% pull()\n\nOR_users_20 <- tl21 %>% dplyr::filter(City==\"Portland_OR\" & year==2020) %>% dplyr::select('Users') %>% pull()\n\nOR_uptick <- OR_users_21 - OR_users_20\n\nnonOR  <- tl21 %>% dplyr::filter(City!=\"Portland_OR\") %>% dplyr::select('Users', year)\nnonOR_gt250k  <- tl21 %>% dplyr::filter(pop_gt250k==TRUE) %>% dplyr::select('Users', year)\n\nnonOR_users_21 <- mean(nonOR$Users[nonOR$year==2021])\nnonOR_users_20 <- mean(nonOR$Users[nonOR$year==2020])\n\nnonOR_uptick <- (nonOR_users_21 - nonOR_users_20)/nonOR_users_20\n\nnonOR_gt250k_users_21 <- mean(nonOR_gt250k$Users[nonOR_gt250k$year==2021])\nnonOR_gt250k_users_20 <- mean(nonOR_gt250k$Users[nonOR_gt250k$year==2020])\n\nnonOR_gt250k_uptick <- (nonOR_gt250k_users_21 - nonOR_gt250k_users_20)/nonOR_gt250k_users_20\n\n\n\nBounding : Maximum impact/minimum cost (subject to random variation)\n(This duplicates the  hand-calculated results in Gitbook HERE)\n3\nWe have a….\n\nlower bound on cost of $13.07 per user ($10.28 per visit) if ‘all visits were generated by the ad’, and\na somewhat more reasonable $24.69 cost per user if Portland was the ‘same as last year’. But it seems most reasonable to allow Portland to have similar trends as other cities.\n\n\n\nDifference in Differences comparison to other cities\nGuiding assumptions:\n\nthe cities used are fairly representative\n‘uptick as a percentage’ is unrelated to city size/visits last year\nall the cities in the comparison group are ‘informative to the counterfactual’ in proportion to their total number of sessions.4\n\n\n\nThus\n112.5% visits uptick (Year on Year) for Portland in 2020\nFor ‘all North American cities other than Portland (with greater than 250,000 people )’:\nThe average is 46.5 users in the 2020 period and 64.5 users in the 2021 period, an uptick of 64.5 - 46.5)/46.5 = about 38.8%. 5\n38.8% uptick \\(\\times\\) 144 = 55.9 ‘counterfactual uptick’ in users for Portland\n162 -55.9 = 106 ‘uptick relative to counterfactual’\n\n\nCode\nor_uptick_vs_cfl_gt250 <- OR_users_21 - OR_users_20 - nonOR_gt250k_uptick*OR_users_20\n\n\nUSD 4000 /106 = $ 37.7 cost per user\nThis seems realistic at a first-pass.\n\n\n\n\nPlotting the ‘difference in difference’\nScatterplot of unique users by city, both time periods, US cities with 500k-1.5mill population, Portland highlighted\n\n\nusers_by_year\n(\n  users_by_year <-  tl21 %>%\n  filter(pop2022 >= 500000 & pop2022 < 1500000) %>%\nggplot() +\n  aes(x = pop2022, y = Users, label = City) +\n  geom_point(aes(shape = factor(year)), size=3, alpha=0.4) +\n  geom_path(aes(group = City)) +\n  geom_label(data = subset(tl21, City %in% c('Portland_OR','Boston','Seattle', 'San Francisco', 'Atlanta', 'San Diego', 'Indianapolis', 'Fort Worth', 'Sacramento', 'Denver', 'San Jose', 'Las Vegas') & year==2020), alpha  = 0.3, size =2.5) +\n  scale_x_continuous(trans='log10') +\n  theme_minimal()\n)\n\n\n\n\n\nusers_by_year\n#users_by_year %>%  plotly::ggplotly()\n\n\nPlotting increases in users by population…\n\n\nincrease_by_size\n(\n  increase_by_size <-  tl21 %>%\n      filter(pop2022 >= 500000 & pop2022 < 1500000 & year==2021) %>%\n   ggplot() +\n  aes(x = pop2022, y = diff_users, label = City) +\n  geom_point(size=3, alpha=0.4) +\n  geom_label(data = subset(tl21, City %in% c('Portland_OR','Boston','Seattle', 'San Francisco', 'Atlanta', 'San Diego', 'Indianapolis', 'Fort Worth', 'Sacramento', 'Denver', 'San Jose', 'Las Vegas')), alpha  = 0.3, size =2.5) +\n  scale_x_continuous(trans='log10') +\n  theme_minimal()\n)\n\n\n\n\n\nPlotting proportional increases in users by 2020 users…\n\n\nCode\n(\n  prop_increase_by_size <-  tl21 %>%\n      filter(pop2022 >= 500000 & pop2022 < 1500000 & year==2021 & lag_users>0) %>%\n   ggplot() +\n  aes(x = lag_users, y = prop_diff_users, label = City) +\n  geom_point(size=3, alpha=0.4) +\n    geom_smooth(method='lm') +\n  geom_text(hjust=0, vjust=0, alpha=.5) +\n  scale_x_continuous(trans='log10') +\n  theme_minimal() +\n    ggtitle(\"Mid-sized cities, proportional changes\") +\n     xlab(\"Users in 2020\") +\n     ylab(\"Proportional change in users\")\n)\n\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "chapters/tlycs_input_simple_analysis.html#next-steps-if-warranted",
    "href": "chapters/tlycs_input_simple_analysis.html#next-steps-if-warranted",
    "title": "5  TLYCS Portland trial: Brief report",
    "section": "5.5 Next steps, if warranted",
    "text": "5.5 Next steps, if warranted\nThe above comparisons are crude and have limitations:\n\nAll cities are weighted equally, no matter their size or similarity to Portland\nSome year-to-year idiosynchratic variation may be unrelated to trends or to the trial. We have not ‘quantified this uncertainty’\n\nIf we want a more precise estimate and careful CIs6 we can build an explicit model and simulation. But I want to know the value of precision here before I dig deeper."
  }
]