[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EA Market testing data analysis",
    "section": "",
    "text": "The “EA Market Testing Team” represents a small group of researchers and practitioners interested in promoting effective altruism and effective charitable giving and learning more about what motivates this.\nSee ‘Effective Giving & Action: Market testing & Synthesis’ - public and private version, and our FAQ here.\n\nThe present web book aims to present all of the trials and experiments that we are able to share. We aim to present the setup, results, and analysis as concisely and transparently as possible. This dynamic document combines data, code, visualization, tests, and limited narrative. It is a complement to the ‘Gitbook’ linked above, and each refer to each other.\n\n\nThis is styled with Quatro. See my template repo here, hosted here,\nHowever, we’re not sure yet if and how it can be integrated with the https://app.gitbook.com/ content.\nNote: as this is Quarto and not Rmd, packages need to be loaded in every chapter. I’ll put these in code/shared_packages_code.R."
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html",
    "href": "chapters/oftw_upsell_input_first_analysis.html",
    "title": "1  OftW pre-giving-tuesday-email upselling split test (considering ‘impact vs emotion’)",
    "section": "",
    "text": "In December 2021, OftW sent out a sequence of emails to existing OftW pledgers/participants asking them for an additional donation. Ther e were two ‘treatment variants’; an emotional email and a standard impact-based email. The treatment was constant by individual (the same person always got emails with the same theme.\nThe details are presented in our (currently private) gitbook HERE and in the linked pre-registration (also on OSF)."
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#capturing-data",
    "href": "chapters/oftw_upsell_input_first_analysis.html#capturing-data",
    "title": "1  OftW pre-giving-tuesday-email upselling split test (considering ‘impact vs emotion’)",
    "section": "1.2 Capturing data",
    "text": "1.2 Capturing data\nKennan and Chloe captured the data and Metadata from\n\nThe OFTW database\nSurveyMonkey\n\nPutting this into the (private) Google sheet linked HERE\nWe added some metadata/explainers to that data.1"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#input-and-clean-data",
    "href": "chapters/oftw_upsell_input_first_analysis.html#input-and-clean-data",
    "title": "1  OftW pre-giving-tuesday-email upselling split test (considering ‘impact vs emotion’)",
    "section": "1.3 Input and clean data",
    "text": "1.3 Input and clean data\nWe input the sheets from the external Google sheet location (access required)…\n\n\nCode\ngs4_auth(scope = \"https://www.googleapis.com/auth/drive\")\ndrive_auth(token = gs4_token())\n\n#Mailchimp data \n\noftw_21_22_mc <- read_sheet(\"https://docs.google.com/spreadsheets/d/1iUKXkEqoadBgtUG_epBzUzCgA_J5mdWWstcXtpAdNJs/edit#gid=521638649\", \n  sheet=\"Raw data (Mailchimp)\")  %>% \n  select(-`Treatment group`) %>%  #remove an un-useful repeated name column \n    mutate(`Treatment Group` = purrr::map_chr(`Treatment Group`, ~ .[[1]])) %>%\n  select(`Email Address`, `Email Date`, `Treatment Group`, Opens, `Donate link clicks`, everything()) %>% #Most relevant features organized first\n  arrange(`Email Address`)\n\n\n#later: remove features brought in from OFTW database, reconstruct it\n\noftw_21_22_mc %>% names() %>% paste(collapse=\", \")\n\n\n[1] \"Email Address, Email Date, Treatment Group, Opens, Donate link clicks, sheet_descriptor, First Name, Last Name, Email Number, Class Year, Donor status, Total Given, Donation amount, Donation frequency, Start string, Platform, Portfolio string, Class lead, Impact 1, Impact 2, Impact 3, Employer, Pledge string, School string, Pledge year, Cancellation Type, Donation Amount String, OFTW matching, OFTW match amount, Corporate match amount, Bonuses announced, Post-bonus contact date, Email Preferences, Start Date, Lives Saved, Member Rating, Record rank, Total Giving Season contributions, Total Giving Season contribution amount\"\n\n\nCode\n#...input descriptors for the above (do this later from \"doc: ...Mailchimp\" sheet\n\n\n\n\nCode\n#Donations data (and OftW database info)\n\noftw_21_22_db_don <- read_sheet(\n  \"https://docs.google.com/spreadsheets/d/1iUKXkEqoadBgtUG_epBzUzCgA_J5mdWWstcXtpAdNJs/edit#gid=521638649\", \n  sheet=\"Giving Season contributions (BigQuery)\") %>% \n  mutate(`Treatment group` = purrr::map_chr(`Treatment group`, ~ .[[1]])) %>%\n    select(`email`, `primary_email`, `donation_date`, `Net_Contribution_Amount_USD`, `payment_platform`, everything()) %>%  #Most relevant features organized first\n  filter(donation_date>=as.Date(\"2021-11-15\")) # At least for now, remove pre-treatment donation data\n\noftw_21_22_db_don %>% names() %>% paste(collapse=\", \")\n\n\n[1] \"email, primary_email, donation_date, Net_Contribution_Amount_USD, payment_platform, Treatment group, Number of email opens, oftw_partner, school, chapter_type, portfolio, contribution_frequency, pledge_date, pledge_start_date, pledge_end_date, donor_status, cancellation_type, pledge_amount, pledge_contribution_frequency, x\"\n\n\n\n\n1.3.1 Labeling and cleaning\nThe code …\n…makes names snake_case, using original names as labels…\n\n\nCode\nlabelled::var_label(oftw_21_22_mc) <- names(oftw_21_22_mc) \nnames(oftw_21_22_mc) <- snakecase::to_snake_case(names(oftw_21_22_mc)) \n\nlabelled::var_label(oftw_21_22_db_don) <- names(oftw_21_22_db_don) \nnames(oftw_21_22_db_don) <- snakecase::to_snake_case(names(oftw_21_22_db_don)) \n\n\n\n…and anonymizes the data, hashing anything with the chance of being identifying\n\n\nCode\nsalty_hash <- function(x) {\n  salt(.seed = 42, x) %>% hash(.algo = \"crc32\")\n}\n\noftw_21_22_mc <- oftw_21_22_mc %>%\n  dplyr::select(-first_name, -last_name) %>%\n    mutate(across(c(email_address,  employer, school_string), salty_hash))\n\n  \noftw_21_22_db_don <- oftw_21_22_db_don %>%\n      mutate(across(c(primary_email, email, school), salty_hash))\n\n\nRoll up to 1 per person; summarize and pivot_wider\n\n\nCode\noutcomes_base_mc <- c(\"opens\", \"donate_link_clicks\")\n\n\noftw_21_22_mc_wide <- oftw_21_22_mc %>%\n  mutate(treat_emotion = case_when(\n    treatment_group == \"1.000000\" ~ 0,\n    treatment_group == \"2.000000\" ~ 1\n  )) %>%\n  group_by(email_address) %>%\n  mutate(across(all_of(outcomes_base_mc), sum, .names = \"{.col}_tot\")) %>%\n  tidyr::pivot_wider(names_from = email_number,\n    values_from = c(\"opens\", \"donate_link_clicks\")) %>%\n  mutate(d_click_don_link = donate_link_clicks_tot > 0) %>%\n  arrange(email_address) %>%\n  filter(row_number() == 1)\n\noftw_21_22_db_don <- oftw_21_22_db_don %>%\n  ungroup() %>%\n  group_by(email) %>%\n  mutate(\n    don_tot = sum(net_contribution_amount_usd),\n    num_don = n(),\n    d_don = num_don > 0,\n    don_tot_ot = sum(net_contribution_amount_usd[contribution_frequency ==\n        \"One-time\"]),\n    num_don_ot = sum(contribution_frequency == \"One-time\"),\n    d_don_ot = num_don_ot > 0,\n    #WAIT THIS IS NOT WIDE DATA -- don't interpret it as 'number of individuals'\n    don_tot_ss = sum(net_contribution_amount_usd[payment_platform == \"Squarespace\"]),\n    num_don_ss = sum(payment_platform == \"Squarespace\"),\n    d_don_ss = num_don_ss > 0,\n  )\n\n\noftw_21_22_db_don_persons <- oftw_21_22_db_don %>%  \n  arrange(email) %>%\n  filter(row_number()==1)     %>%\nmutate(\n      treat_emotion = case_when(\n        treatment_group==\"1.000000\" ~ 0,\n        treatment_group == \"2.000000\" ~ 1)\n    ) \n\n\noftw_mc_db <- power_full_join(oftw_21_22_mc_wide,\n   oftw_21_22_db_don_persons,  by = c(\"email_address\" = \"email\"), conflict = coalesce_xy) %>%\n   mutate(across(starts_with(\"d_don\"), ~replace(., is.na(.), 0)), #make it a 0 if it's not in the donation database\n   d_open= if_else(!is.na(treat_emotion),1,0)\n  )\n\n\noftw_mc_db_openers <- oftw_mc_db %>% \n  filter(!is.na(treat_emotion))\n\n# oftw_21_22_db_don_wide <- oftw_21_22_db_don %>%\n#   select(email, donation_date, net_contribution_amount_usd, payment_platform) %>% \n#    group_by(email) %>%\n#    mutate(row = row_number()) %>%\n#       tidyr::pivot_wider(names_from = row, values_from = c(\"donation_date\", \"net_contribution_amount_usd\"))\n\n\nPrelim results ::: {.cell}\n\nCode\noftw_mc_db %>% tabyl(treat_emotion, d_don)\n\n\n treat_emotion    0   1\n             0 1139 273\n             1  968 231\n            NA    0 395\n\n\nCode\noftw_mc_db %>% tabyl(treat_emotion, d_don_ss)\n\n\n treat_emotion    0 1\n             0 1404 8\n             1 1190 9\n            NA  391 4\n\n\nCode\noftw_21_22_db_don_persons %>% tabyl(treat_emotion, d_don_ot)\n\n\n treat_emotion FALSE TRUE NA_\n             0   248   15  10\n             1   215   12   4\n            NA   328   59   8\n\n\nCode\n#todo: simple statistical measures along with this\n\n:::\n\n\n1.3.2 Constructing outcome measures, especially ‘donations likely driven by email’\n\nDonations (presence, count, amounts) in giving seasons, 1 row per user (with treatment etc.)\n\n\noverall\nnon-regular\n‘likely from email’\n\nare in Giving Season contributions (BigQuery)\n\nsubset for payment platform = squarespace (unlikely to come from any other checkout page)\nemail as primary key, link to Raw Data (mailchimp), filter on ‘Donate link clicks`>0 (note that one needs aggregating by donor because it is ’per email’)\n\n\nGiving season donations ..\n\nGiving Season contributions (BigQuery), sum donation_date Net_Contribution_Amount_USD with filters for one-time etc\nCan check against fields ‘Total Giving Season contributions Total Giving Season contribution amount’\n\n\n\nCode\n#list/matrix of rates for later use\n\noc_mat <- oftw_mc_db %>% \n  mutate(d_open=n()) %>% \n  group_by(treat_emotion) %>% \n  dplyr::summarise(across(starts_with(\"d_\"), ~sum(.x, na.rm = TRUE), .names = \"tot_{.col}\"))\n\noc_mat_r <- oc_mat %>% filter(!is.na(treat_emotion)) #where treatment observed \n\nassigned_emails <- c(2000, 2000) #I was told that about 4000 emails were sent, 2000 to each group"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#descriptives-and-exploratory-analysis",
    "href": "chapters/oftw_upsell_input_first_analysis.html#descriptives-and-exploratory-analysis",
    "title": "1  OftW pre-giving-tuesday-email upselling split test (considering ‘impact vs emotion’)",
    "section": "1.4 Descriptives and exploratory analysis",
    "text": "1.4 Descriptives and exploratory analysis\nNotes:2\n\n1.4.1 Donation and outcome summary statistics, by treatment"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#basic-tests-donation-incidence-and-amounts",
    "href": "chapters/oftw_upsell_input_first_analysis.html#basic-tests-donation-incidence-and-amounts",
    "title": "1  OftW pre-giving-tuesday-email upselling split test (considering ‘impact vs emotion’)",
    "section": "1.5 Basic tests: Donation incidence and amounts",
    "text": "1.5 Basic tests: Donation incidence and amounts\n(See preregistration – go through preregistered tests one at a time. Note that given the observed conversion rates I do not expect any ‘significant differences’.)"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#basic-tests-clicks-and-retention-outcomes",
    "href": "chapters/oftw_upsell_input_first_analysis.html#basic-tests-clicks-and-retention-outcomes",
    "title": "1  OftW pre-giving-tuesday-email upselling split test (considering ‘impact vs emotion’)",
    "section": "1.6 Basic tests: Clicks and retention outcomes",
    "text": "1.6 Basic tests: Clicks and retention outcomes\nI’m following the approach discussed in the ‘RP methods discussion’ under “Significance and equivalence testing” with randomization inference/simulation; building to Bayes\nWe see a ‘small difference’ between treatment groups and it is ‘not significant in standard tests’ (tests not shown here yet). But can we put meaningful bounds on this? Can we statistically ‘rule out large effects’?\n(This parallels the analysis done in HERE, which includes some further explanation of the methods)\n\n\n\n1.6.1 Difference between two binomial random variables: Bayesian binomial test\n\n\nCode\n#would need to generate 'fill in data' if we want to use bayesAB, which requires actual vectors\n\n#add blank rows for 'assigned'\n\nblank_impact <- as_tibble(matrix(NA, nrow = assigned_emails[1]- oc_mat_r$tot_d_open[1], ncol = NCOL(oftw_mc_db)))\n\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if `.name_repair` is omitted as of tibble 2.0.0.\nUsing compatibility `.name_repair`.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\nCode\nnames(blank_impact) <- names(oftw_mc_db)\n\nblank_impact %<>% \n  mutate(across(starts_with(\"d_\"), ~ifelse(is.na(.), 0, 0)),\n    treat_emotion=0)\n  \nblank_emotion <- as_tibble(matrix(NA, nrow = assigned_emails[1]- oc_mat_r$tot_d_open[2], ncol = NCOL(oftw_mc_db)))\nnames(blank_emotion) <- names(oftw_mc_db)\n\nblank_emotion %<>% \n  mutate(across(starts_with(\"d_\"), ~ifelse(is.na(.), 0, 0)),\n    treat_emotion=1)\n\noftw_mc_db_assigned <- \n    bind_rows(oftw_mc_db, blank_impact, blank_emotion) %>%\n  filter(!is.na(treat_emotion))\n\noftw_mc_db_assigned %>% tabyl(treat_emotion)\n\n\n treat_emotion    n percent\n             0 2000     0.5\n             1 2000     0.5\n\n\nOpens by treatment:\n\n\nCode\n# Following r https://www.sumsar.net/blog/2014/06/bayesian-first-aid-prop-test/  \n# alt: http://frankportman.github.io/bayesAB/ \n\n\n#opens_by_treat_fit <- bayes.prop.test(oc_mat_r$tot_d_open, assigned_emails, cred.mass = 0.95) #here I highlight the 95% bounds because it's a strong effect!\n\n#plot(opens_by_treat_fit)\n\nunif_prior <- c('alpha' = 1, 'beta' = 1)\n \nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_open), 'beta' = sum(assigned_emails))\n\n\n#same with AB  package\n# Fit bernoulli test\nopens_by_treat_AB <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_open[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_open[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\nopens_by_treat_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_open[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_open[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\nplot(opens_by_treat_AB) \n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(opens_by_treat_AB_empir) \n\n\n\n\n\n\n\n\n\n\n\nCode\n#WTF -- I need to advance this by command prompt!?\n\n#AB1 <- bayesTest(oftw_21_22_db_don_persons$d_don_ot[trea], B_binom, priors = c('alpha' = 65, 'beta' = 200), n_samples = 1e5, distribution = 'bernoulli')\n\n\n\n\n‘Some donation’ by treatment (only for those who opened, otherwise donations are surely undercounted for Emotion treatment)\n\n\nCode\noftw_21_22_db_don_persons %>% tabyl(treat_emotion, d_don)\n\n\n treat_emotion TRUE\n             0  273\n             1  231\n            NA  395\n\n\nCode\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_don), 'beta' = sum(oc_mat_r$tot_d_open))\n\n(\n  don_by_treat_opened_fit <- bayes.prop.test(oc_mat_r$tot_d_don, oc_mat_r$tot_d_open, cred.mass = 0.80) # 80% credible interval for decision-making purposes\n)\n\n\n\n    Bayesian First Aid proportion test\n\ndata: oc_mat_r$tot_d_don out of oc_mat_r$tot_d_open\nnumber of successes:   273,  231\nnumber of trials:     1412, 1199\nEstimated relative frequency of success [80% credible interval]:\n  Group 1: 0.19 [0.18, 0.21]\n  Group 2: 0.19 [0.18, 0.21]\nEstimated group difference (Group 1 - Group 2):\n  0 [-0.02, 0.02]\nThe relative frequency of success is larger for Group 1 by a probability\nof 0.515 and larger for Group 2 by a probability of 0.485 .\n\n\nCode\nplot(don_by_treat_opened_fit)\n\n\n\n\n\nCode\n#same with AB  package\n# Fit bernoulli test\ndon_by_treat_AB <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don[oftw_mc_db_openers$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\ndon_by_treat_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don[oftw_mc_db_openers$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\nplot(don_by_treat_AB) \n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(don_by_treat_AB_empir) \n\n\n\n\n\n\n\n\n\n\n\nThus we put 80% probability on the difference between the donation rates being no more than (hard-coded) .023/.19 = 12% in either direction. Note that this is not terribly narrowly bounded.\n\n\nNext, for one-time donations only; again as a share of opens\n\n\nCode\n(\n  don_ot_by_treat_opened_fit <- bayes.prop.test(oc_mat_r$tot_d_don_ot, oc_mat_r$tot_d_open, cred.mass = 0.80) # 80% credible interval for decision-making purposes\n)\n\n\n\n    Bayesian First Aid proportion test\n\ndata: oc_mat_r$tot_d_don_ot out of oc_mat_r$tot_d_open\nnumber of successes:    15,   12\nnumber of trials:     1412, 1199\nEstimated relative frequency of success [80% credible interval]:\n  Group 1: 0.011 [0.0075, 0.015]\n  Group 2: 0.011 [0.0068, 0.014]\nEstimated group difference (Group 1 - Group 2):\n  0 [-0.0048, 0.0056]\nThe relative frequency of success is larger for Group 1 by a probability\nof 0.55 and larger for Group 2 by a probability of 0.45 .\n\n\nCode\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_don_ot), 'beta' = sum(oc_mat_r$tot_d_open))\n\n#same with AB  package\n# Fit bernoulli test\ndon_ot_by_treat_AB <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don_ot[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don_ot[oftw_mc_db_openers$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\ndon_ot_by_treat_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don_ot[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don_ot[oftw_mc_db_openers$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\nplot(don_ot_by_treat_AB) \n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(don_ot_by_treat_AB_empir) \n\n\n\n\n\n\n\n\n\n\n\nCode\ndon_ot_by_treat_AB %>% summary(credInt=0.9)\n\n\nQuantiles of posteriors for A and B:\n\n$Probability\n$Probability$A\n         0%         25%         50%         75%        100% \n0.002702340 0.009300256 0.011076158 0.013056869 0.028481793 \n\n$Probability$B\n         0%         25%         50%         75%        100% \n0.002578133 0.008699817 0.010557270 0.012664164 0.031399003 \n\n\n--------------------------------------------\n\nP(A > B) by (0)%: \n\n$Probability\n[1] 0.5518\n\n--------------------------------------------\n\nCredible Interval on (A - B) / B for interval length(s) (0.9) : \n\n$Probability\n       5%       95% \n-0.434144  0.969118 \n\n--------------------------------------------\n\nPosterior Expected Loss for choosing A over B:\n\n$Probability\n[1] 0.1618676\n\n\nCode\ndon_ot_by_treat_AB_lift_int80 <- don_ot_by_treat_AB %>% summary(credInt=0.8)\n\n\ndon_ot_by_treat_AB_lift_int80_empir <- don_ot_by_treat_AB_empir %>% summary(credInt=0.8)\n\n\n80% credible intervals with the uniform prior for the ‘lift’ of Impact relative to Emotion are\n-0.351, 0.707\nand for the empirically informed (but symmetric prior):\n-0.234, 0.357\n(Hard-coded) Here there is just a trace of suggestive evidence that the emotion treatment performed worse. But even our 80% bounds are very wide.\n\n\nFor ‘Squarespace donations only’; these are the donations that plausibly came from the email. First as a share of opens for each treatment :\n\n\nCode\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_don_ss), 'beta' = sum(oc_mat_r$tot_d_open))\n\n\n(\n  don_ss_by_treat_opened_fit <- bayes.prop.test(oc_mat_r$tot_d_don_ss, oc_mat_r$tot_d_open, cred.mass = 0.80) # 80% credible interval for decision-making purposes\n)\n\n\n\n    Bayesian First Aid proportion test\n\ndata: oc_mat_r$tot_d_don_ss out of oc_mat_r$tot_d_open\nnumber of successes:     8,    9\nnumber of trials:     1412, 1199\nEstimated relative frequency of success [80% credible interval]:\n  Group 1: 0.0062 [0.0033, 0.0085]\n  Group 2: 0.0081 [0.0047, 0.011]\nEstimated group difference (Group 1 - Group 2):\n  0 [-0.0062, 0.0023]\nThe relative frequency of success is larger for Group 1 by a probability\nof 0.282 and larger for Group 2 by a probability of 0.718 .\n\n\nCode\n#same with AB  package\n# Fit bernoulli test\ndon_ss_by_treat_AB <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don_ss[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don_ss[oftw_mc_db_openers$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\ndon_ss_by_treat_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_openers$d_don_ss[oftw_mc_db_openers$treat_emotion==0],\n  oftw_mc_db_openers$d_don_ss[oftw_mc_db_openers$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\n\nplot(don_ss_by_treat_AB) \n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(don_ss_by_treat_AB_empir) \n\n\n\n\n\n\n\n\n\n\n\nCode\ndon_ss_by_treat_AB %>% summary(credInt=0.9)\n\n\nQuantiles of posteriors for A and B:\n\n$Probability\n$Probability$A\n          0%          25%          50%          75%         100% \n0.0009902386 0.0048437368 0.0061367516 0.0076340142 0.0191421384 \n\n$Probability$B\n         0%         25%         50%         75%        100% \n0.001462728 0.006437438 0.008042132 0.009891706 0.025724099 \n\n\n--------------------------------------------\n\nP(A > B) by (0)%: \n\n$Probability\n[1] 0.27906\n\n--------------------------------------------\n\nCredible Interval on (A - B) / B for interval length(s) (0.9) : \n\n$Probability\n        5%        95% \n-0.6515304  0.6391667 \n\n--------------------------------------------\n\nPosterior Expected Loss for choosing A over B:\n\n$Probability\n[1] 0.5346835\n\n\nCode\ndon_ss_by_treat_AB_lift_int80 <- don_ss_by_treat_AB %>% summary(credInt=0.8)\n\n\ndon_ss_by_treat_AB_lift_int80_empir <- don_ss_by_treat_AB_empir %>% summary(credInt=0.8)\n\n\n80% credible intervals with the uniform prior for the ‘lift’ of Impact relative to Emotion are\n-0.583, 0.383\nand for the empirically informed (but symmetric prior):\n-0.367, 0.304\n(Hard-coded) Again, even our 80% bounds are very wide.\n\n\nFinally, we consider the above as a share of total emails sent, allowing that ‘opens’ is non-random,\n… and also implicitly assuming that the only impact of these treatments could be on the Squarespace donations made by someone who did open the email.\n\n\nCode\n(\n  don_ss_by_treat_opened_fit_all <- bayes.prop.test(oc_mat_r$tot_d_don_ss, assigned_emails,\n    cred.mass = 0.80) # 80% credible interval for decision-making purposes\n)\n\n\n\n    Bayesian First Aid proportion test\n\ndata: oc_mat_r$tot_d_don_ss out of assigned_emails\nnumber of successes:     8,    9\nnumber of trials:     2000, 2000\nEstimated relative frequency of success [80% credible interval]:\n  Group 1: 0.0043 [0.0025, 0.0062]\n  Group 2: 0.0049 [0.0029, 0.0067]\nEstimated group difference (Group 1 - Group 2):\n  0 [-0.0033, 0.0022]\nThe relative frequency of success is larger for Group 1 by a probability\nof 0.405 and larger for Group 2 by a probability of 0.595 .\n\n\nCode\n# \n#same with AB package\n\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_don_ss), 'beta' = sum(assigned_emails))\n\n\n# Fit Bernoulli test\ndon_ss_by_treat_all_AB <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_don_ss[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_don_ss[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\ndon_ss_by_treat_all_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_don_ss[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_don_ss[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\nplot(don_ss_by_treat_all_AB) \n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(don_ss_by_treat_all_AB_empir) \n\n\n\n\n\n\n\n\n\n\n\nCode\ndon_ss_by_treat_all_AB %>% summary(credInt=0.9)\n\n\nQuantiles of posteriors for A and B:\n\n$Probability\n$Probability$A\n          0%          25%          50%          75%         100% \n0.0006591401 0.0034229384 0.0043367467 0.0053948239 0.0139389081 \n\n$Probability$B\n          0%          25%          50%          75%         100% \n0.0008503472 0.0038633646 0.0048302873 0.0059443205 0.0137935894 \n\n\n--------------------------------------------\n\nP(A > B) by (0)%: \n\n$Probability\n[1] 0.4076\n\n--------------------------------------------\n\nCredible Interval on (A - B) / B for interval length(s) (0.9) : \n\n$Probability\n        5%        95% \n-0.5874267  0.9235146 \n\n--------------------------------------------\n\nPosterior Expected Loss for choosing A over B:\n\n$Probability\n[1] 0.353655\n\n\nCode\ndon_ss_by_treat_all_AB_lift_int80 <- don_ss_by_treat_all_AB %>% summary(credInt=0.8)\n\n\ndon_ss_by_treat_all_AB_lift_int80_empir <- don_ss_by_treat_all_AB_empir %>% summary(credInt=0.8)\n\n\n80% credible intervals with the uniform prior for the ‘lift’ of Impact relative to Emotion are\n\n\nCode\n op(don_ss_by_treat_all_AB_lift_int80$interval$Probability)\n\n\n     10%      90% \n\"-0.507\" \" 0.621\" \n\n\nand for the empirically informed (but symmetric prior):\n\n\nCode\nop(don_ss_by_treat_all_AB_lift_int80_empir$interval$Probability)\n\n\n     10%      90% \n\"-0.329\" \" 0.379\" \n\n\nHard-coded: Here there is almost no evidence in either direction. However, our 80% credible intervals remain wide.\n\n\nUnfortunately, this experiment proved to be underpowered, at least for the donation outcome.\nBut what about clicks on the ‘donation link’? This could arguably be seen as a measure of ‘desire and intention to donate’, and thus might be a more fine-grained and less noisy measure, improving our statistical power.\nSome quick crosstabs (here as a share of opens)\n\n\nCode\n(\n  donclick_by_treat <-  oftw_mc_db %>% \n  filter(!is.na(treat_emotion)) %>% \n  tabyl(treat_emotion, d_click_don_link) %>%\n    adorn_percentages(\"row\") %>%\n    adorn_pct_formatting() %>%\n    adorn_ns() %>%\n    adorn_title() %>% \n    kable(caption =\"Click on donation link by treatment; all opened emails\")  %>% \n  kable_styling(latex_options = \"scale_down\")\n)\n\n\n\nClick on donation link by treatment; all opened emails\n \n  \n     \n    d_click_don_link \n     \n     \n  \n \n\n  \n    treat_emotion \n    FALSE \n    TRUE \n    NA_ \n  \n  \n    0 \n    97.5% (1376) \n    2.1% (29) \n    0.5% (7) \n  \n  \n    1 \n    94.6% (1134) \n    4.7% (56) \n    0.8% (9) \n  \n\n\n\n\n\nIf this is our metric, it only seems fair to take into account ‘whether they opened the email’ as part of this effect. Thus, considering clicks as a share of total emails sent…\n\n\nCode\n(\n  click_don_by_treat_opened_fit <- bayes.prop.test(oc_mat_r$tot_d_click_don_link, assigned_emails,\n    cred.mass = 0.95) # 80% credible interval for decision-making purposes\n)\n\n\n\n    Bayesian First Aid proportion test\n\ndata: oc_mat_r$tot_d_click_don_link out of assigned_emails\nnumber of successes:    29,   56\nnumber of trials:     2000, 2000\nEstimated relative frequency of success [95% credible interval]:\n  Group 1: 0.015 [0.0098, 0.020]\n  Group 2: 0.028 [0.021, 0.036]\nEstimated group difference (Group 1 - Group 2):\n  -0.01 [-0.023, -0.0046]\nThe relative frequency of success is larger for Group 1 by a probability\nof 0.002 and larger for Group 2 by a probability of 0.998 .\n\n\nCode\nplot(click_don_by_treat_opened_fit)\n\n\n\n\n\n\n\nCode\n#same with AB  package\n\nempir_prior <- c('alpha' = sum(oc_mat_r$tot_d_click_don_link), 'beta' = sum(assigned_emails))\n\noftw_mc_db_assigned <- oftw_mc_db_assigned %>% dplyr::filter(!is.na(d_click_don_link))\n\n# Fit bernoulli test\ndon_click_by_treat_all_AB <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_click_don_link[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_click_don_link[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = unif_prior,\n                 distribution = 'bernoulli')\n\ndon_click_by_treat_all_AB_empir <- bayesAB::bayesTest(\n  oftw_mc_db_assigned$d_click_don_link[oftw_mc_db_assigned$treat_emotion==0],\n  oftw_mc_db_assigned$d_click_don_link[oftw_mc_db_assigned$treat_emotion==1],\n                 priors = empir_prior,\n                 distribution = 'bernoulli')\n\nplot(don_click_by_treat_all_AB) \n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(don_click_by_treat_all_AB_empir) \n\n\n\n\n\n\n\n\n\n\n\nCode\ndon_click_by_treat_all_AB %>% summary(credInt=0.9)\n\n\nQuantiles of posteriors for A and B:\n\n$Probability\n$Probability$A\n         0%         25%         50%         75%        100% \n0.005613925 0.013111564 0.014855512 0.016785020 0.030921759 \n\n$Probability$B\n        0%        25%        50%        75%       100% \n0.01457222 0.02602580 0.02846221 0.03102267 0.04764887 \n\n\n--------------------------------------------\n\nP(A > B) by (0)%: \n\n$Probability\n[1] 0.00155\n\n--------------------------------------------\n\nCredible Interval on (A - B) / B for interval length(s) (0.9) : \n\n$Probability\n        5%        95% \n-0.6412379 -0.2472723 \n\n--------------------------------------------\n\nPosterior Expected Loss for choosing A over B:\n\n$Probability\n[1] 0.9686057\n\n\nCode\ndon_click_by_treat_all_AB_lift_int80 <- don_click_by_treat_all_AB %>% summary(credInt=0.8)\n\ndon_click_by_treat_all_AB_lift_int80_empir <- don_click_by_treat_all_AB_empir %>% summary(credInt=0.8)\n\n\n80% credible intervals with the uniform prior for the ‘lift’ of Impact relative to Emotion are\n-0.610, -0.304\nand for the empirically informed (but symmetric prior):\n-0.3119, -0.0531\n(Hard-coded)\nThere is fairly strong evidence that the emotion email lead to a higher rate of clicks on the donation link; note that this even is in spite of the lower rate of email opens.\nThis suggests (IMO) it is worth testing this further.\n\n\n1.6.2 Redoing a bunch of stuff manually\nFirst, some building blocks;\nthe probability distribution over the absolute value of differences between two binomial random variables\nAdapting formulas from Stackexchange post discussion\nDefining their code for this function: ::: {.cell}\n\nCode\nmodBin <- dbinom #DR: I just do this renaming here for consistency with the rest ... but the modBin they defined was redundant as it's already built ins\n\ndiffBin <- function(z, n1, p1, n2, p2){\n\n  prob <- 0\n\n  if (z>=0){\n    for (i in 1:n1){\n      prob <- prob + modBin(i+z, n1, p1) * modBin(i, n2, p2)\n    }\n\n  }\n  else\n  {\n    for (i in 1:n2){\n      prob<-prob+modBin(i+z, n1, p1)*modBin(i, n2, p2)\n    }\n  }\n  return(prob)\n}\n\n:::\n\n\nWe generate an alternate version to cover ‘differences in one direction, i.e., but ’probability of observing (d1-d2)/(n1+n2) share more of d1 responses than d2 responses given sample sizes n1 and n2… over a range of true probabilities p1 and p2’\nthe probability distribution for differences between two binomial random variables in one direction\n\n\n\nFor the present case\nHard-coded notes…\n::: {.foldable}\n\n\n\nCode\nn1 <- oc_mat_r$tot_d_open[1]\nn2 <- oc_mat_r$tot_d_open[2]\nd1 <- oc_mat_r$tot_d_click_don_link[1]\nd2 <- oc_mat_r$tot_d_click_don_link[2]\nz <- d1-d2 #impact minus emotion\n\n\nComputation for a few ‘ad-hoc cases’ (later explore the space with vectors of values)\n\nSuppose truly equal incidence, at the mean level\n\n\n\nCode\np1 <- (d1+d2)/(n1+n2)\n\np2 <- p1\n\n(\n  db_0 <- diffBin(z, n1, p1, n2, p2)\n)\n\n\n[1] 3.963627e-05\n\n\nThis implies there is a 0.00396% chance of getting this exact difference of +-27 incidence(s) between the treatments (in one direction), if the true incidence rates were equal.\nLet’s plot this for a range of ‘incidence rate differences’ in this region. (Sorry, using the traditional plot, ggplot is better).\n\n\nCode\ns <- seq(-10*z, 10*z)\np<-sapply(s, function(z) diffBin(z, n1, p1, n2, p2))\nplot(s,p)\n\n\n\n\n\nWe see a large likelihood of values in the range of the +-27 difference observed, and a low likelihood of a difference of 10 or more in either direction.\n\n\n1.6.3 Adaptation: ‘of this magnitude or smaller’\n\n\nCode\nltmag_diffBin <- function(z, n1, p1, n2, p2){\n  prob <- 0\n  z_n <- -z #negative value\n\n  for (i in z_n:z){     #sum for all integer differences between observed value and its negative, inclusive\n    prob <- prob + diffBin(i, n1, p1, n2, p2)\n    }\n\n  return(prob)\n}\n\n\nNow, a similar computation as above, but for ‘a difference this big or smaller in magnitude’:\n\n\nCode\n  (\n    mag_db_0 <- ltmag_diffBin(z, n1, p1, n2, p2)\n  )\n\n\n[1] 0.9880585\n\n\nThis implies there is a 98.8% chance of getting a difference no larger than this one in magnitude of +/–27 incidences between the treatments if the true incidence rates were equal.\n\n\nAnd finally, what we were looking for: the chance of ‘a difference this small or smaller’ as a function of the true difference…\n(Think about: should we do this for ‘a difference in the same direction’ instead?)\nSet up an arbitrary vector of ‘true differences’ \nBelow, I plot\nY-axis: ’how likely would be a difference in donations ‘as small or smaller in magnitude’” than we see in the data against…\nX-axis: if the “true difference in incidence rates” were of these magnitudes\n(Note: this should cover ‘a difference in either direction’; the probability of a difference in the direction we do see is obviously somewhat smaller)\n\n\nCode\noptions(scipen=999)\n\nB <- c(1, 1.5, 2, 2.5, 3)\n\np1 <- rep((d1+d2)/(n1+n2), length(B))\np2 <- p1*B\n\n\nas.list(ltmag_diffBin(z, n1, p1, n2, p2)*100) %>% format(digits=3, scientific=FALSE)\n\n\n[1] \"98.8\"   \"93.2\"   \"33.6\"   \"1.81\"   \"0.0157\"\n\n\nCode\nprobmag <- ltmag_diffBin(z, n1, p1, n2, p2)\n\n\n#qplot(B, probmag, log  = \"x\", xlab = \"True relative incidence\", ylab =\"Prob. of difference this small\")\n\n(\n  probmag_plot <-\n    ggplot() +\n  aes(x=B, y=probmag) +\n  geom_point() +\n  scale_x_continuous(trans='log2') +\n    ylim(0,1) +\n    xlab(\"True relative incidence rate\") +\n    ylab(\"Prob. diff. as small as obsd\")\n\n)\n\n\n\n\n\nHard-coded takeaways 15 Dec 2021 :\nOur data is consistent with ‘no difference’ (of course) … but its also consistent with ‘a fairly large difference in incidence’\nE.g., even if one treatment truly lead to ‘twice as many donations as the other’, we still have a 20% chance of seeing a differences as small as the one we see (of 8 versus 6)\nWe can reasonably ‘rule out’ differences of maybe 2.5x or greater\nMain point: given the rareness of donations in this context, our sample size doesn’t let us make very strong conclusions in either directions … at least not yet. I hope that combined with other evidence, we will be able to infer more\n\n\n1.6.4 Quick redo assuming equal shares recieved each email, and treating ‘email reciepts as denom’\nApprox 4000 total emails sent?\nFor squarespace\n\n\nCode\nn1 <- 2000\nn2 <- 2000\nd1 <- 10\nd2 <- 9\nz <- d1-d2\n\nB <- c(1/3, 1/2.5, 1/2, 1/1.5, 1, 1.5, 2, 2.5, 3)\n\np1 <- rep((d1+d2)/(n1+n2), length(B))\np2 <- p1*B\n\n\n\n(\n    mag_db_0_ss <- ltmag_diffBin(z, n1, p1, n2, p2)\n  )\n\n\n[1] 0.071197896 0.100275616 0.147727269 0.220724651 0.272100392 0.154218060\n[7] 0.046306831 0.009174658 0.001345637\n\n\nCode\nprobmag_ss <- ltmag_diffBin(z, n1, p1, n2, p2)\n\n\n(\n  probmag_plot_ss <-\n    ggplot() +\n  aes(x=B, y=probmag_ss) +\n  geom_point() +\n  scale_x_continuous(trans='log2') +\n    ylim(0,.51) +\n    xlab(\"True relative incidence rate\") +\n    ylab(\"Prob. diff. as small as obsd\")\n\n)\n\n\n\n\n\nCode\n#note that it is not symmetric bc (I think) a very low incidence on one side makes particular large observed proportional differences more likely\n\n\nFor all one-time donations\n\n\nCode\nn1 <- 2000\nn2 <- 2000\nd1 <- 15\nd2 <- 12\nz <- d1-d2\n\nB <- c(1/3, 1/2.5, 1/2, 1/1.5, 1, 1.5, 2, 2.5, 3)\n\np1 <- rep((d1+d2)/(n1+n2), length(B))\np2 <- p1*B\n\n\n(\n    mag_db_0 <- ltmag_diffBin(z, n1, p1, n2, p2)\n  )\n\n\n[1] 0.0922168538 0.1395427545 0.2249330548 0.3745240308 0.5032003013\n[6] 0.2505329859 0.0521354769 0.0059810731 0.0004413855\n\n\n(the below halts on build, so I commented it out)\n\n\nCode\n(\n  probmag_plot_ot <-\n    ggplot() +\n  aes(x=B, y=probmag) +\n  geom_point() +\n  scale_x_continuous(trans='log2') +\n    ylim(0,.51) +\n    xlab(\"True relative incidence rate\") +\n    ylab(\"Prob. diff. as small as obsd\")\n\n)"
  },
  {
    "objectID": "chapters/oftw_upsell_input_first_analysis.html#demographics-and-interactions-to-do",
    "href": "chapters/oftw_upsell_input_first_analysis.html#demographics-and-interactions-to-do",
    "title": "1  OftW pre-giving-tuesday-email upselling split test (considering ‘impact vs emotion’)",
    "section": "1.7 Demographics and interactions (to do)",
    "text": "1.7 Demographics and interactions (to do)"
  },
  {
    "objectID": "chapters/tlycs_placeholder.html",
    "href": "chapters/tlycs_placeholder.html",
    "title": "2  The Life You Can Save trial: redacted (empty)",
    "section": "",
    "text": "As we have not (yet) been given explicit permission to share the details of the trial with The Life You Can Save, we are not hosting it in the public version"
  },
  {
    "objectID": "chapters/gwwc_gg.html",
    "href": "chapters/gwwc_gg.html",
    "title": "3  Giving What We Can: Giving guides",
    "section": "",
    "text": "Details in Gitbook HERE and Gdoc here\n\n\nCode\nknitr::include_url(\"https://effective-giving-marketing.gitbook.io/untitled/partner-organizations-and-trials/gwwc/giving-guides-+s\")"
  },
  {
    "objectID": "chapters/gwwc_gg.html#capturing-data",
    "href": "chapters/gwwc_gg.html#capturing-data",
    "title": "3  Giving What We Can: Giving guides",
    "section": "3.2 Capturing data",
    "text": "3.2 Capturing data"
  },
  {
    "objectID": "chapters/gwwc_fb.html",
    "href": "chapters/gwwc_fb.html",
    "title": "4  Giving What We Can: Feb 22 Facebook Message Test",
    "section": "",
    "text": "Details in Gitbook HERE and Gdoc here\n\n\nCode\nknitr::include_url(\"https://effective-giving-marketing.gitbook.io/untitled/partner-organizations-and-trials/gwwc/feb-22-message-test\")"
  },
  {
    "objectID": "chapters/gwwc_fb.html#capturing-data",
    "href": "chapters/gwwc_fb.html#capturing-data",
    "title": "4  Giving What We Can: Feb 22 Facebook Message Test",
    "section": "4.2 Capturing data",
    "text": "4.2 Capturing data\n\n\n\n\n\n\nBringing in the data Erin coded\n\n\n\n\n\nAs a start, I source build work (Erin’s work, which I edited a bit) to bring in (and store) the data. I would do the coding a bit differently (more ‘tidyverse’ and less repetition), but it may not be worth redoing at this point.\n\n\n\n\n\nCode\n#this seems to be what Erin used ... but what is \n\nsource(here(\"gwwc\", \"build_GWWC_Feb_22_Message_test.R\"))"
  }
]